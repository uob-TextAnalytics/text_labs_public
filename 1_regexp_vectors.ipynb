{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics Lab 1: Regular Expressions and Vector Representations\n",
    "\n",
    "### Learning Outcomes\n",
    "* Be able to set up a Python and Jupyter notebook environment for text analytics.\n",
    "* Understand how to use regular expressions to preprocess text.\n",
    "* Know how to carry out text normalisation including lemmatisation.\n",
    "* Know how to obtain bigram and TF-IDF vector representations of documents and term-document matrices.\n",
    "* Be able to compute cosine similarity to compare vector representations. \n",
    "\n",
    "### Outline\n",
    "\n",
    "1. Getting started: how to set up your environment, Jupyter notebooks introduction\n",
    "1. Acquiring raw text data\n",
    "1. Regular expressions\n",
    "1. Text normalisation \n",
    "1. Term-document matrices\n",
    "1. Cosine Similarity\n",
    "1. TF-IDF and bigram vectors\n",
    "\n",
    "### How To Complete This Lab\n",
    "\n",
    "Read the text and the code then look for 'TODOs' that instruct you to complete some missing code. Look out for 'QUESTIONS' which you should try to answer before moving on to the next cell. Aim to work through the lab during the scheduled lab hours. To get help, you can talk to TAs or the lecturer during the labs, post questions to the Blackboard discussion board or on Teams, or ask a question in the lectures. \n",
    "\n",
    "The labs *will not be marked*. However, they will prepare you for the coursework, so try to keep up with the weekly labs and have fun with the exercises! Check the textbook (Jurafsky and Martin) for more information on the methods implemented here.\n",
    "\n",
    "### Copilot and other AI tools\n",
    "\n",
    "If you are using an IDE like Visual Studio, we recommend switching off AI tools like Copilot while you are doing the lab. This is because the AI assistant will attempt to generate the answers for you -- sometimes it will be right, and you won't learn anything, and sometimes it will be wrong, and you'll just be confused!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting Started\n",
    "\n",
    "### Setting up your environment\n",
    "\n",
    "We recommend using ```conda``` to create an environment with the correct versions of all the packages you need for these labs. You can install either Anaconda or Miniconda, which will include the ```conda``` program. \n",
    "\n",
    "We provide a .yml file that lists all the packages you will need, and the versions that we have tested the labs with. You can use this file to create your environment as follows.\n",
    "\n",
    "1. Open a terminal. Use the command line to navigate to the directory containing this notebook and the file ```crossplatform_environment.yml```. You can use the command ```cd``` to change directory on the command line.\n",
    "\n",
    "1. For Lab machines only (e.g., in MVB 2.11 and QB 1.80): Load the Anaconda module: ```module load anaconda/3-2024```.\n",
    "\n",
    "1. Run the conda program by typing ```conda env create -f crossplatform_environment.yml```, then answer any questions that appear on the command line.\n",
    "\n",
    "1. Activate the environment by running the command ```conda activate text_analytics```.\n",
    "\n",
    "1. Install some libraries that are not available through Conda: ```pip install bertopic umap-learn```.\n",
    "\n",
    "1. Make kernel available in Jupyter: ```python -m ipykernel install --user --name=text_analytics```.\n",
    "\n",
    "1. Relaunch Jupyter: shutdown any running instances, and then type ```jupyter lab``` into your command line.\n",
    "\n",
    "1. Find this notebook and open it up again.\n",
    "\n",
    "1. Go to the top menu and change the kernel: click on 'Kernel'--> 'Change kernel' --> text_analytics.\n",
    "\n",
    "You should now be ready to go!\n",
    "\n",
    "The core libraries we will be using in this unit are:\n",
    "\n",
    "- [Datasets](https://huggingface.co/docs/datasets/), produced by HuggingFace, is a hub for lots of interesting text datasets.\n",
    "- [NLTK](https://www.nltk.org), a comprehensive NLP library.\n",
    "- [Scikit-learn](https://scikit-learn.org/stable/user_guide.html), for machine learning and classifier evaluation.\n",
    "- [Gensim](https://radimrehurek.com/gensim/), for topic modelling.\n",
    "- [Transformers](https://huggingface.co/docs/transformers/en/index), for state-of-the-art NLP models. \n",
    "- [PyTorch](https://pytorch.org/), a framework for deep learning. \n",
    "- [BERTopic](https://maartengr.github.io/BERTopic/getting_started/quickstart/quickstart.html) for clustering documents into topics.\n",
    "\n",
    "The libraries above have good documentation, which is available either online (links above) or via Python itself, e.g. `help(numpy.array)` in the Python interpreter. \n",
    "\n",
    "### Refreshers for Python and Jupyter\n",
    "\n",
    "If you need a refresher on Python, see the [Introduction to Python lab](https://github.com/UoB-COMS21202/lab_sheets_public/tree/master/lab_1) or the University of Bristol [Beginning Python](https://milliams.gitlab.io/beginning_python/) course. If you are a beginner with Python, you might also like to look at Chapter 1 in the NLTK book, which also provides a guide for \"getting started with Python\": https://www.nltk.org/book/. \n",
    "\n",
    "The labs will be run on [Jupyter Notebook](http://jupyter.org/), an interactive coding environment embedded in a webpage supporting various programing languages (Python, R, Lua, etc.) through the concept of kernels. The code in a notebook is arranged in _cells_. To edit an already existing cell simply double-click on it. Cells can be run by hitting `shift+enter` when editing a cell or by clicking on the `Run` button at the top. Create new cells with the keyboard shortcut `esc` followed by `A` or `B`.\n",
    "\n",
    "**Note**: when you run a code cell, all the created variables, implemented functions and imported libraries will be then available to every other code cell. It is commonly assumed that cells will be run in the correct sequence and running them repeatedly or out-of-order may sometimes cause errors. To reset all variables and functions (for debugging) simply click `Kernel > Restart` from the Jupyter menu.\n",
    "\n",
    "#### Markdown \n",
    "\n",
    "Markdown cells (like this one) allow you to write fancy comments in Markdown format - double click on this cell to see the source. An introduction to Markdown syntax can be found [here](https://daringfireball.net/projects/markdown/syntax). You can also display simple $\\LaTeX$ equations in Markdown thanks to `MathJax` support: for inline equations wrap your equation between `$` symbols; for display mode equations use `$$`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Acquiring Raw Text Data\n",
    "\n",
    "Now, let's get some text data! [HuggingFace's datasets hub](https://huggingface.co/datasets) is a repository of many different text datasets: they are useful for experimenting with NLP tasks and training models. For this lab, we'll start with the IMDB dataset, which contains movie reviews along with their classification into \"positive\" or \"negative\" sentiment. Run the code below to download the data from [HuggingFace's datasets hub](https://huggingface.co/datasets/imdb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "# The data is already divided into training and test sets.\n",
    "# Load the training set:\n",
    "train_dataset = load_dataset(\n",
    "    \"imdb\", # name of the dataset collection\n",
    "    split=\"train\",  # train or test\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")\n",
    "\n",
    "train_dataset = np.random.choice(train_dataset, 100, replace=False)  # we'll only use a subset of the data in this lab so that the code runs quicker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the documents in the dataset like elements in a list. For example, the document with index 3 looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO-DO 1:** Print the label for document 31. What does the value mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*** WRITE YOUR ANSWER HERE ***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regular Expressions\n",
    "\n",
    "In text analytics, we aim to retrieve or extract information from text documents, or classify or summarise documents to better understand a large amount of text. Typically, we are not just looking for a single word or phrase: that can be useful for retrieving documents given a keyword query, but there are many cases where we want to recognise more complex and variable patterns. For example, if we want to find dates, we cannot list all the possible combinations of digits we want to search for, but we can look for patterns of numbers in date format. To do this, we need a way to represent the patterns we are looking for inside a piece of text. The most direct way to represent text patterns is to use regular expressions. Regular expressions provide a standard language for writing text patterns, which we will learn about below. \n",
    "\n",
    "## 2.1 Search\n",
    "\n",
    "We'll start by trying out some simple regular expressions. Suppose we want to identify tweets where people discuss really loved about certain movies. We could start by looking for tweets that contain the word 'love'. Before we try to look for more general patterns, a first step is just to look for all occurrences of the word 'love'. Review the code below to see how we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # Python regular expressions library\n",
    "\n",
    "all_matches = []\n",
    "\n",
    "for review in train_dataset:\n",
    "    matches = re.findall('love', review['text'])\n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has given us a list of matches in the variable `all_matches`, which all contain the string 'love', but not the sentences themselves.\n",
    "This isn't very useful, but we can do better if we define the right regular expression!\n",
    "\n",
    "Regular expressions represent patterns, rather than specific strings, allowing us to generalise our search and retrieve a many different strings that match the pattern.\n",
    "In Python, we differentiate a regular expression from a normal string by putting an 'r' character in front of the string.\n",
    "\n",
    "We can generalise our search by using a _disjunction_, which will match against any one of a set of characters. The disjunction is written inside square brackets. \n",
    "\n",
    "Let's try to retrieve instances of the word \"love\" followed by any letter. We can write a disjunction that matches any lower case letter as `[a-z]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "\n",
    "for review in train_dataset:\n",
    "    matches = re.findall(r'love [a-z]', review['text'])\n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our current search only matches a single letter of the word after 'love'. The length of that following word is variable, so how can we write an expression to match the whole word? \n",
    "\n",
    "Here, we can use a special character, '\\*', which will match against zero or more repetitions of the preceding regular expression. Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "\n",
    "for review in train_dataset:\n",
    "    matches = re.findall(r'love [a-z]*', review['text'])\n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we only want to retrieve the word following 'love', not the string containing 'love ' itself. \n",
    "We can do this using parentheses to create _groups_ of characters, such as this: `([a-z]*)`. The resulting matches will be returned as tuples of groups, and any characters not inside parentheses will not be returned as part of any group. Try out the code below to see this, and note that the space character after 'love' is not returned in the matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "\n",
    "for review in train_dataset:\n",
    "    matches = re.findall(r'(love) ([a-z]*)', review['text'])\n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match) \n",
    "\n",
    "for match in set(all_matches):  # just print the following\n",
    "    print(match[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to retrieve the preceding words as well. It would be better to match capital letters as well as lower case, which we can do with the disjunction `[a-zA-Z]`. \n",
    "\n",
    "**TO-DO 2:** complete the code below to retrieve only the words that precede and follow 'love', including capitalised and lower case words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "\n",
    "for review in train_dataset:\n",
    "    \n",
    "    ### WRITE YOUR CODE HERE\n",
    "\n",
    "    ########\n",
    "    \n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is starting to look more useful, but we still want to retrieve whole sentences. \n",
    "\n",
    "Sentences in English are usually demarcated by punctuation (this is not the same for languages in other scripts, such as Chinese, Hindi and Thai). As we're working with English text only at the moment, let's use the following punctuation marks to identify sentence boundaries: '.', '!', '?'. In the regular expression language, those punctuation marks are special characters that do not literally represent the symbols '.', '!', or '?'. To force Python to interpret them literally, we need to put the escape character '\\\\' in front of them. \n",
    "\n",
    "Now, we can write a disjunction that matches against the punctuation like this: `[\\.\\!\\?]`.\n",
    "\n",
    "So far, we have assumed the text consists only of letters. Can you think of any characters we have excluded here? \n",
    "\n",
    "It can be hard to list every character we want to match. A better way to find all matches could be to use _negation_ to match against any character _except_ the punctuation marks that bound the sentences. A negation will match any character except those specified, which we can write like this: `[^\\.\\!\\?]`, where the '^' indicates the negation.\n",
    "\n",
    "\n",
    "**TO-DO 3:** Retrieve whole sentences containing 'love'. To do this, modify our previous expression by using negation to match all of the characters except '.', '!', and '?'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "\n",
    "for review in train_dataset:\n",
    "    \n",
    "    ### WRITE YOUR CODE HERE\n",
    "\n",
    "    ########\n",
    "    \n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the results -- does the regular expression correctly return sentences containing 'love'?\n",
    "\n",
    "There are lots more special characters that you can use to form really powerful regular expressions for segmenting, retrieving and substituting text. For your reference, you can find a complete list [here](https://docs.python.org/3/library/re.html#regular-expression-syntax). You can take a look at this list and try to rewrite the expressions above in different ways using the special characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Substitution\n",
    "\n",
    "Besides matching and retrieving pieces of text, regular expressions can also be used to alter text by substituting one string for another. There are many potential uses, for example, to fill in templates by replacing placeholders with dates, filenames or other information. For example, imagine a system for sending automated reminders of doctor's appointments. It may contain a sentence \"This is to remind you of your appointment on DATE at TIME.\". Substitution can be used to replace the strings 'DATE' and 'TIME' with specifica values. \n",
    "\n",
    "Regular expression _substitution_ finds a matching string within a larger piece of text, and replaces it with another string.\n",
    "\n",
    "Let's use this to clean up the text by removing the line break characters.\n",
    "\n",
    "In Python, we can use the re.sub() function, which takes three arguments:\n",
    "1. The expression to match. \n",
    "2. The pattern we should replace it with\n",
    "3. The text to apply the subtitution to. \n",
    "\n",
    "Some of the reviews contain some HTML formatting code, `<br />`, which we can try to remove to clean up the text. We can do this by writing an expression for the first argument of re.sub() that matches '<br />'. Take a look at how this works by running the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ORIGINAL TEXT: ')\n",
    "print(train_dataset[5]['text'])\n",
    "    \n",
    "clean_article = re.sub(r'<br />', r' ', train_dataset[5]['text'])  # replace HTML breaks with a space\n",
    "    \n",
    "print('CLEANER TEXT: ')\n",
    "print(clean_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text Normalisation \n",
    "\n",
    "For most text analytics tasks, such as document classification, we will first need to transform the raw text to a suitable format for input to method such as a classifier. This process is called _text normalisation_ and is part of the _preprocessing_ stage. There are three common steps:\n",
    "\n",
    "1. Sentence segmentation: this is needed when we want to process each sentence separately, e.g., to classify its sentiment. We have already tried out a basic approach to obtaining complete sentences using regular expressions. This would need to be modified to return a list of all sentences in a document. \n",
    "2. Tokenisation, in which the sentences are split into a sequence of tokens, which include words, numbers and punctuation marks.\n",
    "3. Word normalisation, in which different forms of a word are replaced by a root form. Many text analytics models, such as document classifiers, can benefit from words being _normalised_ to consistent word forms (e.g., \"dog\", \"Dog\" and \"dogs\" could all normalised to \"dog\"), as this can reduce the diversity of the vocabulary make it easier to find meaningful patterns in the data. \n",
    "\n",
    "We are now going to see how to perform these steps using the NLTK library.\n",
    "\n",
    "## 3.1 Sentence Segmentation\n",
    "\n",
    "Let's start by using NLTK to split a document into sentences. This should give better results than our regular expressions above.\n",
    "\n",
    "You may get some errors from NLTK when you try to use sent_tokenize or word_tokenize further down. This is usually because you need to download and install some NLTK data. Please check the error message to find out which package is required. You probably need to install packages called 'punkt' and 'wordnet'. You can install these packages by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "review = train_dataset[5]['text']\n",
    "\n",
    "sents = nltk.sent_tokenize(review)\n",
    "\n",
    "for sent in sents:\n",
    "    print(\"<SENTENCE>\")\n",
    "    print(sent)  # print the sentences of this document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO-DO 4:** Use the regular expression substitution code from section 2.2 to remove the '\\<br /\\>' tags from the sentences displayed above and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sents = []\n",
    "\n",
    "for sent in sents:\n",
    "    \n",
    "    ### WRITE YOUR OWN CODE HERE\n",
    "\n",
    "    #######\n",
    "    \n",
    "    print(\"<SENTENCE>\")\n",
    "    print(sent)  # print the sentences of this document\n",
    "    \n",
    "    clean_sents.append(sent)  # save the cleaned sentences for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Tokenisation\n",
    "\n",
    "NLTK provides a similar function for tokenizing the text at the word level. You can find the documentation [here](https://www.nltk.org/api/nltk.tokenize.html). Most tokenizers use either regular expressions or a machine learning model that was trained on a large dataset to learn token-splitting rules. \n",
    "\n",
    "**TO-DO 5:** Use word_tokenize() to tokenize each of the sentences from the last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sents = []\n",
    "\n",
    "for sent in clean_sents:\n",
    "    ### WRITE YOUR OWN CODE HERE\n",
    "\n",
    "    #######\n",
    "    \n",
    "    print(\"<TOKENS>\")\n",
    "    print(tokens)\n",
    "    \n",
    "    tokenized_sents.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to see how NLTK has handled the non-letter characters. \n",
    "* What does it do with most punctuation marks? \n",
    "* When does it not split tokens based on punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in tokenized_sents:\n",
    "    for tok in sent:\n",
    "        if re.search(r'[^a-zA-Z0-9]', tok):  # find the non-letter and non-digit characters\n",
    "            print(tok)  # print the entire token containing the non-letter/non-digit character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Word Normalisation\n",
    "\n",
    "Many words can appear in different forms, including: \n",
    "* Conjugated verbs like \"think\", \"thinks\" and \"thought\",\n",
    "* Plural and singular nouns like \"dog\" and \"dogs\",\n",
    "* Common abbrevations and synonyms like \"USA\" and \"US\". \n",
    "\n",
    "Mapping all of these surface forms to a single root form reduces the size of the vocabulary that we have to deal with and can therefore improve the performance of text classifiers or topic models.\n",
    "\n",
    "The two most widely used tools for this task in English are the Porter Stemmer and WordNet Lemmatizer. These tools apply a series of regular expression substitutions to tokenised text to convert words to a standard format. \n",
    "* The Porter stemmer is much faster but just removes word prefixes and endings, which leads to some errors. It is often used when real-time or high-volume text processing is needed.\n",
    "* As well as applying regular expressions, lemmatizers look words up in a dictionary to find their root forms, so are more accurate but much slower. \n",
    "\n",
    "Let's start by applying the [Porter Stemmer class](https://www.nltk.org/_modules/nltk/stem/porter.html) to our tokenised text by calling the stem() method. The output may look a bit strange, but note that the aim of the stemmer is *not* to produce readable text, but to quickly and efficiently reduce variations of words to a single form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.PorterStemmer() \n",
    "stemmed_sents = []\n",
    "\n",
    "for sent in tokenized_sents:\n",
    "    stemmed_sent = [stemmer.stem(tok) for tok in sent]\n",
    "    \n",
    "    stemmed_sents.append(stemmed_sent)\n",
    "    \n",
    "    print(\"<STEMMED TOKENS>\")\n",
    "    print(stemmed_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the stemming results to lemmatisation. For this task, NLTK provides the [class WordNetLemmatizer](https://www.nltk.org/_modules/nltk/stem/wordnet.html) with the method lemmatize(). This method takes an argument, `pos`, that determines whether the lemmatizer is applied to nouns, verbs, adjectives or adverbs.\n",
    "\n",
    "**TO-DO 6:** Use the WordNetLemmatizer to lemmatize the nouns in the tokenized sentences. Set the `pos` argument to 'n'. \n",
    "\n",
    "**TO-DO 7:** Add a second call to lemmatize() to lemmatize the verbs in the sentences as well. Set the `pos` argument to 'v'. \n",
    "\n",
    "How do the results compare with the Porter stemmer? \n",
    "\n",
    "How have the verbs in the sentences changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer() \n",
    "lemma_sents = []\n",
    "for sent in tokenized_sents:\n",
    "    \n",
    "    ### WRITE YOUR OWN CODE HERE\n",
    "    lemma_sent = \n",
    "    #######\n",
    "    \n",
    "    lemma_sents.append(lemma_sent)\n",
    "    \n",
    "    print(\"<LEMMATIZED TOKENS>\")\n",
    "    print(lemma_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Vector Representations of Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions are great for tasks such as finding specific patterns in text. However, it is not always possible to write a regular expression that captures all the patterns we want to find. For example, suppose we want to classify social media posts into positive and negative sentiment to see if they are favourable towards a particular famous person. We can't write down a pattern to capture all the ways of saying favourable things about that person -- it's way too diverse. \n",
    "\n",
    "Instead, we can use machine learning to learn to recognise a wide range of patterns from a set of examples. To classify a new example, a machine learning classifier requires a *representation* of each piece of text that it can compare against the patterns it has learned. Raw text is not usually a suitable representation, and we usually need a way to turn text data into vectors -- essentially, lists of numbers. Vector representations have several advantages: for example, they map words, sentences and documents to points in a high-dimensional space, so we can learn to separate the space into different regions corresponding to classes; they allow us to compute the similarity between pieces of text by computing distances. \n",
    "\n",
    "In this section, we'll loook at the simplest way to obtain vector representations of words and documents by constructing a *term-document matrix*. A term-document matrix has rows referring to terms, and columns referring to documents. Each element contains a count of how many times a particular term occurred in a particular document. We can treat rows as vector representations of terms, and columns as vector representations of documents.\n",
    "\n",
    "To compute term-document matrices, we need to use the text normalisation steps above. Most importantly, we need to tokenise the text into words (and other types of token) so we can count their occurrences. Normalising the words is often helpful too, as it reduces the number of rows in the matrix and makes it less sparse.\n",
    "\n",
    "We can compute a term-document matrix using the CountVectorizer class from Scikit-learn. By default, this class takes raw text sequences and applies an English tokenizer automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "input_text = [review['text'] for review in train_dataset]  # use a list of sentences as an example. \n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(input_text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you seen the method \"fit()\" before with other Scikit-learn classes? What do you think it does for the CountVectorizer?\n",
    "\n",
    "Looking inside the vectorizer, we can see the vocabulary it has created from the input text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we need to call \"transform()\" to get a term-document matrix. Try it out and find out what it produces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_doc_mat = vectorizer.transform(input_text).T  # transpose so that rows are terms\n",
    "print(term_doc_mat)\n",
    "print(term_doc_mat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO-DO 8:** Use the term-document matrix above to write a function that returns a term vector. Get the term vector for the word 'happy'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR ANSWER HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO-DO 9:** What do the values in the vector mean? This representation is known as a 'bag of words' because it ignores the word order and document structure. Can you think of any disadvantages of representing documents as bags of words? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER = these are the number of times the chosen word (or other token type, such as punctuation mark or number) occured in each document in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Comparing Vectors with Cosine Similarity\n",
    "\n",
    "Vectors representations allow us to compare documents or terms by computing their similarity. This is useful for tasks such as clustering documents into topics, or finding documents that are similar to a 'query' document. \n",
    "In order to compute similarity or distance, we need to represent documents as numerical vectors. \n",
    "\n",
    "The most common way to compare vectors is to compute the cosine of the angles between them. This measures how much the vectors point in the same direction. It ignores their magnitude, which means that shorter documents with lower word counts can be directly compared to long documents with more words. \n",
    "\n",
    "Let's take a term from the IMDB dataset as a 'query' and compare it to two others using cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our first document\n",
    "query_vec = get_term_vector(vectorizer, term_doc_mat, 'happy')\n",
    "\n",
    "# get the second term\n",
    "term2_vec = get_term_vector(vectorizer, term_doc_mat, 'sad')\n",
    "\n",
    "# get a third term\n",
    "term3_vec = get_term_vector(vectorizer, term_doc_mat, 'enjoy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity is defined as:\n",
    "\n",
    "$$similarity<v_1, v_2> = \\frac{v_1 \\cdot v_2}{|| v_1 || \\cdot || v_2 ||}$$\n",
    "\n",
    "**TO-DO 9:** Complete the function below to computes cosine similarity between two vectors. Hint: use Numpy's dot function for the dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossim(vec1, vec2):   \n",
    "    ### WRITE YOUR OWN CODE HERE\n",
    "    \n",
    "    #######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO-DO 10:** Which term do you expect to have higher similarity to the query? Run the code below to use your cosine similarity function, and see if the results meet your expectations.\n",
    "\n",
    "ANSWER: -- The 'enjoy' should be more similar to 'happy' than 'sad'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim1 = cossim(query_vec, term2_vec)\n",
    "print(f'The cosine similarity between the query and term2 is: {cos_sim1}')\n",
    "\n",
    "cos_sim2 = cossim(query_vec, term3_vec)\n",
    "print(f'The cosine similarity between the query and term3 is: {cos_sim2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Bags of N-grams\n",
    "\n",
    "Our representations above were purely bag-of-words representations: they ignored word order and simply counted single tokens, or 'unigrams'. However, word order is important for understanding the meaning of a piece of text. What if we expand our bags of words to also count other _features_ that help us account for word order? Features are any attributes of the text that we can measure; a simple improvement is to count pairs of consecutive tokens, or _bigrams_, to capture phrases as well as individual words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1,2))  # include bigrams as well as unigrams\n",
    "bigram_vectorizer.fit(input_text)  \n",
    "bigram_vectorizer.vocabulary_  # show the vocabulary of the bigram vectorizer, which includes both unigrams and bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a vectorizer that can produce representations including bigrams. Let's apply it to the text to get an expanded term-document matrix:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_doc_mat = bigram_vectorizer.transform(input_text).T\n",
    "print(\"The shape of the bigram document-term matrix is: \", bigram_doc_mat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL: \n",
    "\n",
    "This part aims to give you some more understanding of bigrams and n-grams in general, and shows you how to use the lemmatizer with the CountVectorizer class. It is not required to do this part, and we will revisit the use of n-grams and lemmatizers in the later lab on classifiers. \n",
    "\n",
    "The code below chooses a document that we can experiment with. \n",
    "\n",
    "**TO-DO 11:** Find the top three documents that are most similar to `selected_doc` when using vectors of bigrams+unigrams. Print them out. Hint: numpy contains useful functions such as argsort, for sorting a list or array. \n",
    "\n",
    "**TO-DO 12:** Repeat the process with the pure unigram bag of words representations. Does the list change? Can you see why it may be different? \n",
    "\n",
    "**TO-DO 13:** Experiment with other choices of `selected_doc` and increasing the length of the features bigrams to trigrams and other lengths of n-gram.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_doc = 1\n",
    "scores = []\n",
    "\n",
    "print(input_text[selected_doc])  # print the document we're using as our query\n",
    "print(\"\\n\")\n",
    "\n",
    "### WRITE YOUR OWN CODE HERE \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary size is probably getting very large, now that we are using bigrams and other n-grams. \n",
    "\n",
    "To apply lemmatization, we have to go back to the CountVectorizer and define a new tokenizer class that will carry out the extra step of lemmatization. The code below shows how to apply lemmatization with the CountVectorizer class to reduce the vocabulary size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):  # this 'tokenizer' will also do additional preprocessing steps, namely, lemmatize verbs and adjectives\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.wnl = nltk.WordNetLemmatizer()\n",
    "        \n",
    "    def __call__(self, docs):\n",
    "        return [self.wnl.lemmatize(self.wnl.lemmatize(tok, pos='v'), pos='a') for tok in nltk.word_tokenize(docs)]\n",
    "    \n",
    "lemm_vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(), ngram_range=(1,2), token_pattern=None)  # include bigrams as well as unigrams\n",
    "\n",
    "lemm_vectorizer.fit(input_text)\n",
    "lemm_term_doc_mat = lemm_vectorizer.transform(input_text).T\n",
    "\n",
    "# Print out some of the features in the vocabulary:\n",
    "print(list(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Vocabulary size: {len(vectorizer.vocabulary_)}')\n",
    "print(f'Size of term document matrix with lemmatization: {lemm_term_doc_mat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO-DO 14:** Run the code below and compare with your previous results. Print out the vocabulary to see how the lemmatizer has changed the results. You can also experiment with the 'pos' parameter to lemmatise different categories of word (verbs, adjectives, nouns). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR OWN CODE HERE\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
