{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics Lab 1: Regular Expressions and Classifiers\n",
    "\n",
    "### Learning Outcomes\n",
    "* Be able to set up a Python and Jupyter notebook environment for text analytics\n",
    "* Understand how to use regular expressions to preprocess text\n",
    "* Know how to carry out text normalisation and train and test naïve Bayes and logistic regression classifiers.\n",
    "* Be able to examine learned model parameters and explain how each classifier makes a decision.\n",
    "\n",
    "### Outline\n",
    "\n",
    "1. Getting started: how to set up your environment, Jupyter notebooks introduction\n",
    "1. Acquiring raw text data\n",
    "1. Regular expressions\n",
    "1. Text normalisation \n",
    "1. Training and evaluating naïve Bayes using Scikit-learn.\n",
    "1. Training and evaluating logistic regression using Scikit-learn.\n",
    "1. Optional extension: lemmatization and bigram features.\n",
    "1. Optional extensions: lexicon features.\n",
    "\n",
    "### How To Complete This Lab\n",
    "\n",
    "Read the text and the code then look for 'TODOs' that instruct you to complete some missing code. Look out for 'QUESTIONS' which you should try to answer before moving on to the next cell. Aim to work through the lab during the scheduled lab hours. To get help, you can talk to TAs or the lecturer during the labs, post questions to the Blackboard discussion board or on Teams, or ask a question in the lectures. \n",
    "\n",
    "The labs *will not be marked*. However, they will prepare you for the coursework, so try to keep up with the weekly labs and have fun with the exercises! To understand what's going on inside the methods we use here, make sure to watch the lecture videos for the same week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting Started\n",
    "\n",
    "### Setting up your environment\n",
    "\n",
    "We recommend using ```conda``` to create an environment with the correct versions of all the packages you need for these labs. You can install either Anaconda or Miniconda, which will include the ```conda``` program. \n",
    "\n",
    "We provide a .yml file that lists all the packages you will need, and the versions that we have tested the labs with. You can use this file to create your environment as follows.\n",
    "\n",
    "1. Open a terminal. Use the command line to navigate to the directory containing this notebook and the file ```crossplatform_environment.yml```. You can use the command ```cd``` to change directory on the command line.\n",
    "\n",
    "1. Run the conda program by typing ```conda env create -f crossplatform_environment.yml```, then answer any questions that appear on the command line.\n",
    "\n",
    "1. Activate the environment by running the command ```conda activate text_analytics```.\n",
    "\n",
    "1. Make kernel available in Jupyter: ```python -m ipykernel install --user --name=text_analytics```.\n",
    "\n",
    "1. Relaunch Jupyter: shutdown any running instances, and then type ```jupyter lab``` into your command line.\n",
    "\n",
    "1. Find this notebook and open it up again.\n",
    "\n",
    "1. Go to the top menu and change the kernel: click on 'Kernel'--> 'Change kernel' --> text_analytics.\n",
    "\n",
    "You should now be ready to go!\n",
    "\n",
    "The core libraries we will be using in this unit are:\n",
    "\n",
    "- [Datasets](https://huggingface.co/docs/datasets/), produced by HuggingFace, is a hub for lots of interesting text datasets.\n",
    "- [NLTK](https://www.nltk.org), a comprehensive NLP library.\n",
    "- [Scikit-learn](https://scikit-learn.org/stable/user_guide.html), for machine learning and classifier evaluation.\n",
    "- [Gensim](https://radimrehurek.com/gensim/), for topic modelling.\n",
    "- [Transformers](https://huggingface.co/docs/transformers/en/index), for state-of-the-art NLP models. \n",
    "- [PyTorch](https://pytorch.org/), a framework for deep learning. \n",
    "\n",
    "The libraries above have good documentation, which is available either online (links above) or via Python itself, e.g. `help(numpy.array)` in the Python interpreter. \n",
    "\n",
    "### Refreshers for Python and Jupyter\n",
    "\n",
    "**Skip this part if you're already familiar with Python and Jupyter notebooks.**\n",
    "\n",
    "This lab assumes you have used Python and Jupyter Notebooks before. \n",
    "\n",
    "For an introduction or refresher on Python, see the [Introduction to Python lab](https://github.com/UoB-COMS21202/lab_sheets_public/tree/master/lab_1) or the University of Bristol [Beginning Python](https://milliams.gitlab.io/beginning_python/) course. If you are a beginner with Python, you might also like to look at Chapter 1 in the NLTK book, which also provides a guide for \"getting started with Python\": https://www.nltk.org/book/ \n",
    "\n",
    "The labs will be run on [Jupyter Notebook](http://jupyter.org/), an interactive coding environment embedded in a webpage supporting various programing languages (Python, R, Lua, etc.) through the concept of kernels.  \n",
    "\n",
    "It allows you to enrich your code with complex comments formatted in Markdown and $\\LaTeX$, as well as to place the results of your computation right below your code.\n",
    "\n",
    "Notebooks are organised in cells which can contain either code (in our case, this will be Python code) or text, which can be easily and nicely formatted using the Markdown notation. \n",
    "\n",
    "To edit an already existing cell simply double-click on it. You can use the toolbar to insert new cells, edit and delete them (or use keyboard shortcuts which are very handy to speed up coding). \n",
    "\n",
    "Cells can be run by hitting `shift+enter` when editing a cell or by clicking on the `Run` button at the top. Running a Markdown cell will simply display the formatted text, while running a code cell will execute the commands executed in it. Create new cells with the keyboard shortcut `esc` followed by `A` or `B`.\n",
    "\n",
    "**Note**: when you run a code cell, all the created variables, implemented functions and imported libraries will be then available to every other code cell. It is commonly assumed that cells will be run in the correct sequence and running them repeatedly or out-of-order may sometimes cause errors. To reset all variables and functions (for debugging) simply click `Kernel > Restart` from the Jupyter menu.\n",
    "\n",
    "#### Markdown \n",
    "\n",
    "Markdown cells allow you to write fancy and simple comments: all of this is written in Markdown - double click on this cell to see the source. An introduction to Markdown syntax can be found [here](https://daringfireball.net/projects/markdown/syntax).\n",
    "\n",
    "As Markdown is translated to HTML upon displaying it also allows you to use pure HTML: more details are available [here](https://daringfireball.net/projects/markdown/syntax#html).\n",
    "\n",
    "Finally, you can also display simple $\\LaTeX$ equations in Markdown thanks to `MathJax` support. For inline equations wrap your equation between `$` symbols; for display mode equations use `$$`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Acquiring Raw Text Data\n",
    "\n",
    "Now, let's get some text data! We'll start with the IMDB dataset, which contains movie reviews along with their classification into \"positive\" or \"negative\" sentiment. Run the code below to download the data from [HuggingFace's datasets hub](https://huggingface.co/datasets/imdb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "# The data is already divided into training and test sets.\n",
    "# Load the training set:\n",
    "train_dataset = load_dataset(\n",
    "    \"imdb\", # name of the dataset collection\n",
    "    split=\"train\",  # train or test\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")\n",
    "\n",
    "train_dataset = np.random.choice(train_dataset, 5000, replace=False)  # we'll only use a subset of the data in this lab so that the code runs quicker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the documents in the dataset like elements in a list. For example, document 3 looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regular Expressions\n",
    "\n",
    "## 2.1 Search\n",
    "\n",
    "We'll start by trying out some regular expressions. Suppose we want to identify tweets where people discuss really loved about certain movies. We could start by looking for tweets that contain the word 'love'. A first step would be to find all occurrences of the word 'love'. Review the code below to see how we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # Python regular expressions library\n",
    "\n",
    "all_matches = []\n",
    "\n",
    "for review in train_dataset:\n",
    "    matches = re.findall('love', review['text'])\n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has given us a list of matches in the variable `all_matches`, which all contain the string 'love', but not the sentences themselves.\n",
    "This isn't very useful, but we can do better if we define the right regular expression!\n",
    "\n",
    "Regular expressions define a pattern, rather than a specific string, allowing us to generalise our search and retrieve a many different strings that match the pattern.\n",
    "In Python, we differentiate a regular expression from a normal string by putting an 'r' character in front of the string.\n",
    "\n",
    "We can generalise our search by using a _disjunction_, which will match against any one of a set of characters. The disjunction is written inside square brackets. \n",
    "\n",
    "Let's try to retrieve instances of the word \"merger\" followed by any letter. We can write a disjunction that matches any lower case letter as `[a-z]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "\n",
    "for review in train_dataset:\n",
    "    matches = re.findall(r'love [a-z]', review['text'])\n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our current search only matches a single letter of the word after 'think'. The length of that following word is variable, so how can we write an expression to match the whole word? \n",
    "\n",
    "Here, we can use a special character, '\\*', which will match against zero or more repetitions of the preceding regular expression. Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "\n",
    "for review in train_dataset:\n",
    "    matches = re.findall(r'love [a-z]*', review['text'])\n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we only want to retrieve the word following 'love', not the string containing 'love ' itself. \n",
    "We can do this using parentheses to create _groups_ of characters, such as this: `([a-z]*)`. The resulting matches will be returned as tuples of groups, and any characters not inside parentheses will not be returned as part of any group. Try out the code below to see this, and note that the space character after 'love' is not returned in the matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "\n",
    "for review in train_dataset:\n",
    "    matches = re.findall(r'(love) ([a-z]*)', review['text'])\n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match) \n",
    "\n",
    "for match in set(all_matches):  # just print the following\n",
    "    print(match[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to retrieve the preceding words as well. It would be better to match capital letters as well as lower case, which we can do with the disjunction `[a-zA-Z]`. \n",
    "\n",
    "TODO 1: complete the code below to retrieve only the words that precede and follow 'merger', including capitalised and lower case words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "\n",
    "for review in train_dataset:\n",
    "    \n",
    "    ### WRITE YOUR CODE HERE\n",
    "    matches = \n",
    "    ########\n",
    "    \n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is starting to look more useful, but we still want to retrieve whole sentences. \n",
    "\n",
    "Sentences in English are usually demarcated by punctuation, so let's use the following punctuation marks to identify sentence boundaries: '.', '!', '?'. Those punctuation marks are special characters when used in regular expressions, so to force Python to interpret them literally, we need to put the escape character '\\\\' in front of them. \n",
    "\n",
    "Now, we can write a disjunction that matches against the punctuation like this: `[\\.\\!\\?]`.\n",
    "\n",
    "So far, we have assumed the text consists only of letters. Can you think of any characters we have excluded here? \n",
    "\n",
    "A better way to find all matches would be to use _negation_ to match against any character _except_ the punctuation marks that bound the sentences. A negation will match any character except those specified, which we can write like this: `[^\\.\\!\\?]`, where the '^' indicates the negation.\n",
    "\n",
    "\n",
    "TODO 2: Retrieve whole sentences containing 'love'. To do this, modify our previous expression by using negation to match all of the characters except '.', '!', and '?'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "\n",
    "for review in train_dataset:\n",
    "    \n",
    "    ### WRITE YOUR CODE HERE\n",
    "    matches = \n",
    "    ########\n",
    "    \n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the results -- does the regular expression correctly return sentences containing 'love'?\n",
    "\n",
    "There are lots more special characters that you can use to form really powerful regular expressions for segmenting, retrieving and substituting text. For your reference, you can find a complete list [here](https://docs.python.org/3/library/re.html#regular-expression-syntax)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Substitution\n",
    "\n",
    "We can also use regular expressions during _preprocessing_ to clean up text and prepare it for further analysis. For this, we use regular expression _substitution_, which finds a matching string within a larger piece of text, and replaces it with another string.\n",
    "\n",
    "Let's use this to clean up the text by removing the line break characters.\n",
    "\n",
    "In Python, we can use the re.sub() function, which takes three arguments:\n",
    "1. The expression to match. \n",
    "2. The pattern we should replace it with\n",
    "3. The text to apply the subtitution to. \n",
    "\n",
    "Some of the reviews contain some HTML formatting code, `<br />`, which we can try to remove to clean up the text. We can do this by writing an expression for the first argument of re.sub() that matches '<br />'. Take a look at how this works by running the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ORIGINAL TEXT: ')\n",
    "print(train_dataset[3]['text'])\n",
    "    \n",
    "clean_article = re.sub(r'<br />', r' ', train_dataset[3]['text'])  # replace HTML breaks with a space\n",
    "    \n",
    "print('CLEANER TEXT: ')\n",
    "print(clean_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text Normalisation \n",
    "\n",
    "For most text analytics tasks, we will first need to transform the raw text to a suitable format for input to method such as a classifier. This process is called _text normalisation_ and is part of the _preprocessing_ stage. There are three common steps:\n",
    "\n",
    "1. Sentence segmentation: we have already tried out a basic approach to obtaining complete sentences using regular expressions. This would need to be modified to return a list of all sentences in a document. \n",
    "2. Tokenisation, in which the sentences are split into a sequence of tokens, which include words, numbers and punctuation marks.\n",
    "3. Word normalisation, in which different forms of a word are replaced by a root form.\n",
    "\n",
    "We are now going to see how to perform these steps using the NLTK library.\n",
    "\n",
    "## 3.1 Sentence Segmentation\n",
    "\n",
    "Let's start by using NLTK to split a document into sentences. This should give better results than our regular expressions above.\n",
    "\n",
    "You may get some errors from NLTK when you try to use sent_tokenize or word_tokenize further down. This is usually because you need to download and install some NLTK data. Please check the error message to find out which package is required. You probably need to install packages called 'punkt' and 'wordnet'. You can install these packages by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "review = train_dataset[3]['text']\n",
    "\n",
    "sents = nltk.sent_tokenize(review)\n",
    "\n",
    "for sent in sents[:5]:\n",
    "    print(\"<SENTENCE>\")\n",
    "    print(sent)  # print the first five sentences of this document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 3: Use the regular expression substitution code from section 2.2 to remove the '\\<br /\\>' tags from the sentences displayed above and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sents = []\n",
    "\n",
    "for sent in sents[:5]:\n",
    "    \n",
    "    ### WRITE YOUR OWN CODE HERE\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    print(\"<SENTENCE>\")\n",
    "    print(sent)  # print the first five sentences of this document\n",
    "    \n",
    "    clean_sents.append(sent)  # save the cleaned sentences for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Tokenisation\n",
    "\n",
    "NLTK provides a similar function for tokenizing the text at the word level. You can find the documentation [here](https://www.nltk.org/api/nltk.tokenize.html). Most tokenizers use either regular expressions or a machine learning model that was trained on a large dataset to learn token-splitting rules. \n",
    "\n",
    "TODO 4: Use word_tokenize() to tokenize each of the sentences from the last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sents = []\n",
    "\n",
    "for sent in clean_sents:\n",
    "    ### WRITE YOUR OWN CODE HERE\n",
    "    tokens = \n",
    "    #######\n",
    "    \n",
    "    print(\"<TOKENS>\")\n",
    "    print(tokens)\n",
    "    \n",
    "    tokenized_sents.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to see how NLTK has handled the non-letter characters. \n",
    "* What does it do with most punctuation marks? \n",
    "* When does it not split tokens based on punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in tokenized_sents:\n",
    "    for tok in sent:\n",
    "        if re.search(r'[^a-zA-Z0-9]', tok):  # find the non-letter and non-digit characters\n",
    "            print(tok)  # print the entire token containing the non-letter/non-digit character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Word Normalisation\n",
    "\n",
    "Many words can appear in different forms, including: \n",
    "* Conjugated verbs\n",
    "* Plural and singular nouns\n",
    "* Common abbrevations and synonyms like \"USA\" and \"US\". \n",
    "\n",
    "Mapping all of these surface forms to a single root form reduces the size of the vocabulary that we have to deal with and can therefore improve the performance of text classifiers or topic models.\n",
    "\n",
    "The two most widely used tools for this task in English are the Porter Stemmer and WordNet Lemmatizer. These tools apply a series of regular expression substitutions to tokenised text to convert words to a standard format. \n",
    "* The Porter stemmer is much faster but just removes word prefixes and endings, which leads to some errors. It is often used when real-time or high-volume text processing is needed.\n",
    "* As well as applying regular expressions, lemmatizers look words up in a dictionary to find their root forms, so are more accurate but much slower. \n",
    "\n",
    "Let's start by applying the [Porter Stemmer class](https://www.nltk.org/_modules/nltk/stem/porter.html) to our tokenised text by calling the stem() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.PorterStemmer() \n",
    "stemmed_sents = []\n",
    "\n",
    "for sent in tokenized_sents:\n",
    "    stemmed_sent = [stemmer.stem(tok) for tok in sent]\n",
    "    \n",
    "    stemmed_sents.append(stemmed_sent)\n",
    "    \n",
    "    print(\"<STEMMED TOKENS>\")\n",
    "    print(stemmed_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the stemming results to lemmatisation. For this task, NLTK provides the [class WordNetLemmatizer](https://www.nltk.org/_modules/nltk/stem/wordnet.html) with the method lemmatize(). This method takes an argument, `pos`, that determines whether the lemmatizer is applied to nouns, verbs, adjectives or adverbs.\n",
    "\n",
    "TODO 5: Use the WordNetLemmatizer to lemmatize the nouns in the tokenized sentences. Set the `pos` argument to 'n'. \n",
    "\n",
    "TODO 6: Add a second call to lemmatize() to lemmatize the verbs in the sentences as well. Set the `pos` argument to 'v'. \n",
    "\n",
    "How do the results compare with the Porter stemmer? \n",
    "\n",
    "How have the verbs in the sentences changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer() \n",
    "lemma_sents = []\n",
    "for sent in tokenized_sents:\n",
    "    \n",
    "    ### WRITE YOUR OWN CODE HERE\n",
    "    \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    lemma_sents.append(lemma_sent)\n",
    "    \n",
    "    print(\"<LEMMATIZED TOKENS>\")\n",
    "    print(lemma_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classification\n",
    "\n",
    "In this part, we will introduce text processing tools from the scikit-learn library. Let's start by loading the test set for the IMDB sentiment dataset.\n",
    "\n",
    "## 4.1 Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test set:\n",
    "test_dataset = load_dataset(\n",
    "    \"imdb\",\n",
    "    split=\"test\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Test dataset with {len(test_dataset)} instances loaded\")\n",
    "\n",
    "test_dataset = np.random.choice(test_dataset, 2000, replace=False)  # we'll only use a subset of the data in this lab so that the code runs quicker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to tokenise the text of each tweet and convert it to a bag of words, ready for input to a classifier. \n",
    "To do this, we will use the scikit-learn library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract a bag of words, we can use the CountVectorizer class ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)).\n",
    "This class outputs the bag of words as a feature vector, where the length of the vector is equal to the size of the vocabulary, and the values are the counts of each words in a document. \n",
    "\n",
    "Run the code below to obtain feature vectors for the training and test samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [sample[\"text\"] for sample in train_dataset]\n",
    "train_label = [sample[\"label\"] for sample in train_dataset]\n",
    "test_text = [sample[\"text\"] for sample in test_dataset]\n",
    "test_label =  [sample[\"label\"] for sample in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# CountVectorizer can do its own tokenization, but for consistency we want to\n",
    "# carry on using WordNetTokenizer. \n",
    "vectorizer = CountVectorizer(tokenizer=word_tokenize)  # construct the vectorizer\n",
    "\n",
    "vectorizer.fit(train_text)  # Learn the vocabulary\n",
    "X_train = vectorizer.transform(train_text)  # extract training set bags of words\n",
    "X_test = vectorizer.transform(test_text)  # extract test set bags of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit() method sets the vectorizer up by extracting a vocabulary from some text data. \n",
    "\n",
    "QUESTION: Why do we fit the CountVectorizer on the training set only?\n",
    "\n",
    "The vectorizer stores the vocabulary as a dictionary that maps a token to its index in the feature vector. The code below looks up the indexes of some example words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reprlib\n",
    "\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "print(vocabulary['the'])\n",
    "print(vocabulary['horse'])\n",
    "print(vocabulary['smile'])\n",
    "\n",
    "print(f'Vocabulary size = {len(vocabulary)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Naive Bayes Classifier\n",
    "\n",
    "The code above has obtained the feature vectors and lists of labels. The data is now ready for use\n",
    "with scikit-learn's classifiers.\n",
    "\n",
    "Scikit-learn contains several different variants of naïve Bayes for different kinds of data. For our bag of words data, we need to use the [MultinomialNB class](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB).\n",
    "\n",
    "\n",
    "TODO 7: Look at the documentation for MultinomalNB and write code to train a NB classifier using `X_train` and `train_dataset['label']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a trained model, we would like to evaluate its performance on some test data. \n",
    "\n",
    "TODO 8: Refer to the documentation again and predict the labels for the test set. Use `X_test` as the inputs to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute metrics for classifier performance using [scikit-learn's metrics libary](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules). A useful function for multi-class classification (when there are more than two classes) is the [classification report function](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report).\n",
    "\n",
    "TODO 9: Refer again to the documentation, and compute accuracy, precision, recall and F1 scores on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine the classifier that we learned. You may wish to refer back to the slides on naïve Bayes classifiers or to [Jurafsky and Martin's textbook](https://web.stanford.edu/~jurafsky/slp3/4.pdf) to see how the implementation relates to the equations. \n",
    "\n",
    "Previously, we trained a MultinomialNB classifier. The trained classifier object stores all the probabilities that it learned during training, which are needed to make predictions. The log of the likelihoods of each word given the class are represented by the attribute `feature_log_prob_`. So, if your classifier object is named `classifier`, you can access the log likelihoods with `classifier.feature_log_prob_`.\n",
    "\n",
    "TODO 10: Print out the likelihood of the words 'happy' and 'hate' in each class. Hint: look up the index of the chosen words in `vocabulary`. The rows of `feature_log_prob` correspond to classes, and the columns to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "### CHANGE THE NAME OF THE CLASSIFIER VARIABLE BELOW TO USE YOUR TRAINED CLASSIFIER\n",
    "feat_likelihoods = np.exp(classifier.feature_log_prob_)  # Use exponential to convert the logs back to probabilities\n",
    "###\n",
    "\n",
    "# WRITE YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment classes are negative (0) and positive (1). \n",
    "\n",
    "QUESTION: According to the feature log likelihoods, which class has the strongest association with 'happy' and with 'hate'?\n",
    "\n",
    "A key part of evaluating a classifier is investigating the errors it makes to better understand its limitations. \n",
    "\n",
    "TODO 11: Complete the code below to print out some misclassified reviews along with their predicted and true labels. Can you see any reasons why a NB classiier would make mistakes with these instances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_indexes = y_test_pred != test_label  # compare predictions to gold labels\n",
    "\n",
    "# get the text of reviews where the classifier made an error:\n",
    "reviews_err = np.array(test_text)[error_indexes]\n",
    "\n",
    "# WRITE YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Logistic Regression Classifier\n",
    "\n",
    "Another simple, linear classifier is logistic regression. This classifier does not rely on the conditional independence assumption, so can better model features that are highly correlated with each other. Scikit-learn provides the [logisticRegression class](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), which has a very similar interface to the naïve Bayes classifier.\n",
    "\n",
    "TODO 12: Train a logistic regression classifier, referring to the scikit-learn documentation as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION: How does the performance of logistic regression compare with naïve Bayes?\n",
    "\n",
    "The logistic regression classifier works by learning a weight for each feature that indicates its importance in predicting a class. These weights are stored in the `coef_` attribute of the LogisticRegression object, which has rows corresponding to classes, and columns corresponding to words in the vocabulary. \n",
    "\n",
    "TODO 13: Print out the weights for 'happy' and 'hate' for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION: Are the weights what you would expect to see?\n",
    "\n",
    "The code below prints out the words with the highest weights for the positive class. We use numpy's `argsort` function to get the indexes of the sorted weights. Run the code below to show the result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feats_to_show = 10\n",
    "\n",
    "# Flip the index so that values are keys and keys are values:\n",
    "keys = vectorizer.vocabulary_.values()\n",
    "values = vectorizer.vocabulary_.keys()\n",
    "vocab_inverted = dict(zip(keys, values))\n",
    "\n",
    "strongest_idxs = np.argsort(classifier.coef_[0])[-n_feats_to_show:]\n",
    "\n",
    "for idx in strongest_idxs:\n",
    "    print(f'{vocab_inverted[idx]} with weight {classifier.coef_[0][idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 14: Use the same code as for naïve Bayes to print out examples of misclassified tweets and their labels. Hint: you should be able to copy and paste your code from above :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_indexes = y_test_pred != test_label  # compare predictions to gold labels\n",
    "\n",
    "# get the text of tweets where the classifier made an error:\n",
    "reviews_err = np.array(test_text)[error_indexes]\n",
    "\n",
    "# WRITE YOUR CODE HERE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. OPTIONAL: Lemmatization and N-grams\n",
    "\n",
    "You only need to do this section if you finish the previous sections before the end of the lab.\n",
    "\n",
    "In the previous lab, we tried out lemmatization. This is useful for reducing the size of the vocabulary. Could it help us here?\n",
    "\n",
    "To apply lemmatization, we have to go back to the CountVectorizer and define a new tokenizer class that will carry out the extra step of lemmatization. Run the code below to test this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "class LemmaTokenizer(object):  # this 'tokenizer' will also do additional preprocessing steps, namely, lemmatize verbs and adjectives\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        \n",
    "    def __call__(self, tweets):\n",
    "        return [self.wnl.lemmatize(self.wnl.lemmatize(tok, pos='v'), pos='a') for tok in word_tokenize(tweets)]\n",
    "    \n",
    "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer())\n",
    "\n",
    "vectorizer.fit(train_text)\n",
    "X_train = vectorizer.transform(train_text)\n",
    "X_test = vectorizer.transform(test_text)\n",
    "\n",
    "# Print out some of the features in the vocabulary:\n",
    "print(list(vectorizer.vocabulary_)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Vocabulary size: {len(vectorizer.vocabulary_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 15: Now, repeat your training of the NB classifier using the new features, and compare its performance with the previous classifers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR OWN CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION: Did lemmatization bring about any improvements on this dataset?\n",
    "\n",
    "The bag of words is a very simple representation of the text that does not capture enough information to make accurate sentiment classifications. Another way to improve it could be to use bigrams instead of single words as our features. Bigrams are pairs of words that occur one after another in the text. Bigrams are a kind of 'n-gram', where 'n=2'. \n",
    "\n",
    "To extract bigrams, we again modify our CountVectorizer. This class has a parameter `ngram_range`, which determines the range of sizes of n-grams the vectorizer will include. If we set `ngram_range=(1,1)` we have our standard bag of words. If we set it to `ngram_range=(2,2)`, we use bigrams instead. Choosing If we set `ngram_range=(1,2)` will use both single tokens (unigrams) and bigrams.\n",
    "\n",
    "TODO 16: Create a new CountVectorizer that extracts bigram features instead of unigrams (single tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "print(f'Vocabulary size: {len(vectorizer.vocabulary_)}')\n",
    "\n",
    "# Print out some of the features in the vocabulary:\n",
    "print(list(vectorizer.vocabulary_)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 17: Now, repeat your training of the logistic regression or naïve Bayes classifier using the new features, and compare its performance with the previous classifers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR OWN CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION: Do bigrams improve performance on this dataset?\n",
    "\n",
    "# 6. Optional: Lexicon Features\n",
    "\n",
    "You only need to do this part if you finish the other parts before the end of the lab session. \n",
    "\n",
    "The NLTK library contains sentiment lexicons, which are lists of words with negative or positive connotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the sentiment scores for some words in the lexicon by running the code below. What do the scores mean and why do some words have no score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testwords = ['happy', 'wonderful', 'horrible', 'boring', 'tablecloth', 'not']\n",
    "\n",
    "for word in testwords:\n",
    "    if word in analyser.lexicon:\n",
    "        print(f'{word}: {analyser.lexicon[word]}')\n",
    "    else:\n",
    "        print(f'{word}: NOT IN LEXICON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to use this function to compute counts of all positive and negative words. Let's start by recording whether the words in our vocabulary are positive or negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out some of the features in the vocabulary:\n",
    "print(list(vectorizer.vocabulary_)[:20])\n",
    "\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "lex_pos_scores = np.zeros((1, len(vocabulary)))\n",
    "lex_neg_scores = np.zeros((1, len(vocabulary)))\n",
    "\n",
    "# get the Vader lexicon scores for each word in our vocabulary\n",
    "for i, term in enumerate(vocabulary):\n",
    "    if term in analyser.lexicon and analyser.lexicon[term] > 0:\n",
    "        lex_pos_scores[0, i] = 1\n",
    "    elif term in analyser.lexicon and analyser.lexicon[term] < 0:\n",
    "        lex_neg_scores[0, i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the counts of positive and negative words in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the scores for each instance in the data set. \n",
    "\n",
    "# Multiply the lexicon scores by the feature vectors, then sum over the \n",
    "# vocabulary to get the total positive and total negative counts:\n",
    "lex_pos_train = np.sum(X_train.multiply(lex_pos_scores), axis=1)\n",
    "lex_pos_test = np.sum(X_test.multiply(lex_pos_scores), axis=1)\n",
    "\n",
    "lex_neg_train = np.sum(X_train.multiply(lex_neg_scores), axis=1)\n",
    "lex_neg_test = np.sum(X_test.multiply(lex_neg_scores), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can append the counts to the feature vector and treat them as extra features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "X_train = hstack((X_train, lex_pos_train, lex_neg_train))\n",
    "X_test = hstack((X_test, lex_pos_test, lex_neg_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO-DO 18: Use the new X_train and X_test feature vectors to train and evaluate your classifier. \n",
    "Does adding the lexicon features improve performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR OWN CODE HERE\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_analytics",
   "language": "python",
   "name": "text_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
