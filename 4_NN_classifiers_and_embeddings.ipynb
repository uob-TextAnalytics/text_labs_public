{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba36903-ee71-4a62-9bb8-8e0270fdae64",
   "metadata": {},
   "source": [
    "# Text Analytics Lab 4: Neural Network Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de31fe1-70a5-4357-b9d9-be40038c556c",
   "metadata": {},
   "source": [
    "This notebook is introduces a new library, Pytorch, which we use to construct simple neural networks for processing text. \n",
    "\n",
    "NLP pipelines perform a series of steps to preprocess and then extract features from a piece of text, before inputting these features to a final classifier, sequence labeller, or other output stage. Neural networks allow us to reduce the amount of feature engineering needed in the pipeline, by learning to extract features in the hidden layers. \n",
    "\n",
    "Like logistic regression, they are disciminative models: they learn the classification task directly, without learning to generate the text. However, unlike logistic regression, they can model nonlinear functions of their inputs. This is achieved by having hidden layers with nonlinear activation functions. \n",
    "\n",
    "Some key advantages of neural networks can be summarised as:\n",
    "   * It can model nonlinear functions, so can handle much more complex relationships between features and class labels.\n",
    "   * It performs representation learning: the hidden layers learn how to extract features from low-level data.\n",
    "   * It can process sequences of tokens -- we don't have to think in terms of a single feature vector representing a document as we did for logistic regression.\n",
    "\n",
    "On the other hand, the downsides are:\n",
    "   * Much more expensive to train and test.\n",
    "   * It can overfit very badly to small datasets.\n",
    "   * The features learned by the hidden layers can be hard to interpret, which can make it hard to predict the model's behaviour, e.g., what sort of cases it may fail on.\n",
    "   * Generalisation is unpredictable: they sometimes behave in unexpected ways when applied to test data outside the training set distribution. \n",
    "\n",
    "## Learning Outcomes\n",
    "\n",
    "These sections will contain tutorial-like instructions, as you have seen in previous text analytics labs. On completing these sections, you should be able to...\n",
    "1. Recognise the steps required to train and test a neural text classifier with Pytorch\n",
    "1. Modify the architecture of a neural text classifier by adding new hidden layers and changing hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1a8852-b72a-48dd-a266-25a4bb774f6a",
   "metadata": {},
   "source": [
    "# 1. Load a Dataset\n",
    "\n",
    "First, we will load the [`tweets hate speech` dataset](https://huggingface.co/datasets/tweets_hate_speech_detection). This is a dataset for classifying Twitter posts into hate/non-hate speech. Disciminating hate speech is useful for social media companies to identify posts that they want to screen and potentially hide from other users. It's also useful for social scientists seeking to understand online discussions and the effect of social media on public opinions.  This dataset contains tweets with binary labels, with 1 indicating hate speech and 0 indicating no hate speech. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5519c4b9-1cac-4e8a-9e7e-a8a40b389bdf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import word2vec\n",
    "from gensim.utils import tokenize\n",
    "import gensim.downloader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e599bf09-ed1a-4e40-a551-2e3a889a3487",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"sentiment\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "dev_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")\n",
    "print(f\"Development/validation dataset with {len(dev_dataset)} instances loaded\")\n",
    "print(f\"Test dataset with {len(test_dataset)} instances loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421d24e4-d90d-4f50-a167-5f4876076355",
   "metadata": {},
   "source": [
    "# 2. Introducing Neural Text Classifiers\n",
    "\n",
    "This section shows you how to implement a neural network classifier using Pytorch and leads you through the steps required to process text sequences. Pytorch is an open source machine learning library that implements multi-dimensional arrays (called \"tensors\") and various mathematical operations on those arrays that are need for many machine learning methods. It supports GPU acceleration and is one of two major frameworks used in deep learning (the other is Tensorflow). \n",
    "\n",
    "Let's start by building a neural network text classifier that takes a sequence of tokens as input, and predicts a class label. For simplicity, it will use a single fully connected feedforward layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca4f88e-bd86-4917-8e9d-26ace635873e",
   "metadata": {},
   "source": [
    "\n",
    "We are going to construct the neural network in this form:\n",
    "\n",
    "<img src=\"neural_text_classifier_smaller.png\" alt=\"Neural text classifier diagram\" width=\"600px\"/>\n",
    "\n",
    "The first step -- as always -- is to get our data into the right format. \n",
    "   1. We start by tokenising the raw text data so that each document (in this case, each tweet) is represented as a sequence of text tokens.\n",
    "   2. The neural network cannot process the tokens as strings, so we need to convert each token to a numerical input value. We can create a vocabulary of token types, then convert each token to its index in the vocabulary. Elsewhere, you may have seen tokens represented by one-hot vectors. For PyTorch, it's not necessary to create one-hot vectors for each token, as the library works with the indexes of tokens directly.  \n",
    "   3. In the embedding layer, the indexes of tokens are used to look up the corresponding word embedding from a matrix containing all embeddings.\n",
    "\n",
    "So, let's now implement these steps. We start by tokenising the text using the Gensim tokeniser. We can make use of the \"map()\" function provided by the Dataset class to apply tokenisation to all samples in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c246e0b2-6dc3-4c8e-b09f-3fc7b084a9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_text(sample):\n",
    "    sample[\"tokens\"] = list(tokenize(sample['text']))\n",
    "    return sample\n",
    "\n",
    "tok_train_dataset = train_dataset.map(tok_text)\n",
    "\n",
    "tok_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d7b4ad-dafe-43a1-be0f-3597f53bca8a",
   "metadata": {},
   "source": [
    "Next, we can build our vocabulary of token types using the Dictionary class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77871773-4444-43cd-a967-13141c89468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(tok_train_dataset[\"tokens\"]) # construct word<->id mappings - it does it in alphabetical order\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7294e1be-6677-4d5e-9e54-b24e390ba354",
   "metadata": {},
   "source": [
    "We can look up the ID of a token -- its index in the vocabulary -- using the dictionary object as shown in the cell below. However, we need to reserve the value 0 for a special padding token, so we add one to all the indexes to get their \"input_id\" that we will pass into the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0701d70a-ee50-4357-85ce-574e3538ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.token2id['a'] + 1  # get input_id for the token \"a\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4b5765-f273-4177-8ca9-bde848f2f1f8",
   "metadata": {},
   "source": [
    "**TO-DO 1:** Write a function that maps a sample from tok_train_dataset from a list of tokens to a list of token IDs. Don't forget to add one to the indexes, as in the cell above. Save the list of token IDs into the sample dictionary as 'input_ids'. Then, use the ``map()`` method to apply this to the whole training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a71101-e504-42a8-9862-74796af52cd0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### WRITE YOUR CODE HERE\n",
    "\n",
    "# tokenize training set and convert to input IDs.\n",
    "\n",
    "####\n",
    "\n",
    "# The map method of the dataset object takes a function as its argument, \n",
    "# and applies that function to each document in the dataset.\n",
    "pre_train_dataset = tok_train_dataset.map(encode_text)\n",
    "len(pre_train_dataset)  # length of preprocessed training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce8d8c-ce83-49b7-9b88-b84233be7dbf",
   "metadata": {},
   "source": [
    "Our neural network's input layer has a fixed size, so our input sequences need to all have the same size. This means that all of our documents must have the same number of tokens. Let's plot a histogram to understand the length distribution of the texts in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832da949-3fec-4d26-936e-23c943546b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_l = [len(doc) for doc in pre_train_dataset['input_ids']]\n",
    "print('Mean of the document length: {}'.format(np.mean(rv_l)))\n",
    "print('Median of the document length: {}'.format(np.median(rv_l)))\n",
    "print('Maximum document length: {}'.format(np.max(rv_l)))\n",
    "\n",
    "plt.hist(rv_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da97d92c-7002-4b63-9011-8ed21ea52e95",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now neeed to choose a fixed sequence length, then *pad* the documents that are shorter than this maximum by adding a special token to the start of the sequence. The special pad token has an input value of 0. Any documents that exceed the length will be truncated.\n",
    "\n",
    "**TO-DO 2:** Complete the padding code below to insert 0s at the start of any sequences that are too short, and to truncate any sequences that are too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8488dfa-61b8-438e-92ee-7f9b84b27a68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequence_length = 30  # truncate all docs longer than this. Pad all docs shorter than this.\n",
    "\n",
    "def pad_text(sample):\n",
    "    ###WRITE YOUR OWN CODE HERE\n",
    "\n",
    "\n",
    "    \n",
    "    ##########\n",
    "    return sample\n",
    "\n",
    "# The map method will call pad_text for every document in the dataset\n",
    "pad_train_dataset = pre_train_dataset.map(pad_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fb33a1-ca74-4a95-bbd5-2b9e50167733",
   "metadata": {},
   "source": [
    "We now have our data in almost the right format! To train a model using PyTorch, we are going to wrap our dataset in a [DataLoader object](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). This allows the training process to select random subsets of the dataset -- mini-batches -- which it will use for learning with mini-batch stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab3c226-402b-4eb5-8f9a-fe216b4c5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# convert from the Huggingface format to a TensorDataset so we can use the mini-batch sampling functionality\n",
    "def convert_to_data_loader(dataset, num_classes):\n",
    "    # convert from list to tensor\n",
    "    input_tensor = torch.from_numpy(np.array(dataset['input_ids']))\n",
    "    label_tensor = torch.from_numpy(np.array(dataset['label'])).long()\n",
    "    tensor_dataset = TensorDataset(input_tensor, label_tensor)\n",
    "    loader = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return loader\n",
    "\n",
    "num_classes = len(np.unique(pad_train_dataset[\"label\"]))   # number of possible labels \n",
    "\n",
    "train_loader = convert_to_data_loader(pad_train_dataset, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b4245a-52b8-4fa6-bfeb-75f3a3a704df",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a5825c-4a0e-4fc8-9de6-504bc7227377",
   "metadata": {},
   "source": [
    "Let's preprocess the development and test set as well. We need the dev set when training our network to the model's fit, and to assess its performance with different hyperparameter settings. We use the test set to evaluate the final model on data that was not seen during its development to provide a fairer assessment of the model's generalisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa6ddc-b936-431c-865e-dc55eaf3086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_dev_dataset = dev_dataset.map(tok_text).map(encode_text)\n",
    "pad_dev_dataset = pre_dev_dataset.map(pad_text)\n",
    "dev_loader = convert_to_data_loader(pad_dev_dataset, num_classes)\n",
    "\n",
    "pre_test_dataset = test_dataset.map(tok_text).map(encode_text)\n",
    "pad_test_dataset = pre_test_dataset.map(pad_text)\n",
    "test_loader = convert_to_data_loader(pad_test_dataset, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7b857f-5d57-44fd-a1b4-e1b31a009c72",
   "metadata": {},
   "source": [
    "As shown in the diagram above, we will build a NN with three different layers for sentiment classification.\n",
    "\n",
    "### Embedding layer\n",
    "In the embedding layer, the network will create its own embeddings for the index with a given embedding dimension.\n",
    "The module `nn.Embedding()` creates a simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "This module is often used to store word embeddings and retrieve them using indices.\n",
    "The module's input is a list of indices, and the output is the corresponding word embeddings.\n",
    "\n",
    "[Documentation for Embedding Class](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
    "\n",
    "### Fully-connected layer\n",
    "Fully-connected layers in a neural network are those layers where all the inputs from the previous layer are connected to every unit of the fully-connected layer. Here we will use fully-connected layers for the hidden layer and output layer. In Pytorch this kind of layer is implemented by combining the 'Linear' class with a nonlinear activation function:\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "\n",
    "## Activation functions\n",
    "In Pytorch, the activation function is not included in the Linear class (or other kinds of neural network layer), so we need to explicitly connect each layer to an activation function.\n",
    "In Pytorch, we construct a neural network by connecting up the output of each component to the input of the next, thereby creating a computation graph.\n",
    "To complete the hidden layer, we connect the ouput of the linear layer to a ReLU activation function, thereby creating a nonlinear function.\n",
    "\n",
    "The cell below defines a class for our neural text classifier. The constructor creates each of the layers and the activations. The dimensions of each layer need to be correct so that the output of one layer can be passed as input to the next, but the code is not yet complete.\n",
    "\n",
    "Below the constructor is the forward method. This is called in the 'forward pass' to map the neural network's inputs to its outputs. In PyTorch, we pass data through each layer of the model, connecting them together, then returning the output of the final layer.\n",
    "\n",
    "**TO-DO 3:** Complete the constructor and the forward method below for a NN with three layers. The places where you need to add code are marked in the cell below. Refer to the Pytorch documentation for additional help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a13cf-4b4d-4ac2-868c-8ad5c1b72b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class FFTextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, sequence_length, embedding_size, hidden_size, num_classes):\n",
    "        super(FFTextClassifier, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Here we just need to construct the components of our network. We don't need to connect them together yet.\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_size) # embedding layer\n",
    "        \n",
    "        ### COMPLETE THE CODE HERE: WRITE IN THE MISSING ARGUMENTS SPECIFYING THE DIMENSIONS OF EACH LAYER\n",
    "        self.hidden_layer = nn.Linear() # Fully connected hidden layer\n",
    "        self.activation = nn.ReLU() # Hidden layer\n",
    "        ##########\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes) # Fully connected output layer\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):\n",
    "        # Input dimensions are:  (batch_size, seq_length)\n",
    "        embedded_words = self.embedding_layer(input_words)  # (batch_size, seq_length, embedding_size)\n",
    "\n",
    "        # flatten the sequence of embedding vectors for each document into a single vector.\n",
    "        embedded_words = embedded_words.reshape(embedded_words.shape[0], sequence_length*self.embedding_size)  # batch_size, seq_length*embedding_size\n",
    "\n",
    "        z = self.hidden_layer(embedded_words)   # (batch_size, hidden_size)\n",
    "        \n",
    "        ### ADD THE MISSING LINE HERE\n",
    "\n",
    "        ########\n",
    "\n",
    "        output = self.output_layer(h)                      # (batch_size, num_classes)\n",
    "\n",
    "        # Notice we haven't applied a softmax activation to the output layer -- it's not required by Pytorch's loss function.\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06f22db-b953-4f98-920f-3f9a2b1e3707",
   "metadata": {},
   "source": [
    "Now the class is complete. \n",
    "\n",
    "**TO-DO 4:** In the next cell, create a NN with the FFTextClassifier class we wrote.\n",
    "\n",
    "Hint: `model = FFTextClassifier(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d795a-adbc-4c48-aadf-f911520ac42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(dictionary) + 1  # add one for the padding tokens\n",
    "embedding_size = 25  # number of dimensions for embeddings\n",
    "hidden_size = 16 # number of hidden units\n",
    "\n",
    "###WRITE YOUR OWN CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a6c48d-f2b4-45e6-838e-d08c895f7e86",
   "metadata": {},
   "source": [
    "After desigining our network, we need to create a training function to calculate the loss for each input and perform backpropagation to optimise the network.\n",
    "During training, the weights of all the layers will be updated.\n",
    "\n",
    "Below, we build a training function to train the NN over a fixed number of epochs (an epoch is one iteration over the whole training dataset).\n",
    "The function also prints the performance of both training and development/validation set after each epoch.\n",
    "\n",
    "Here we use cross-entropy loss, which is the standard loss function for classification that we also used for logistic regression. The module `nn.CrossEntropyLoss()` operates directly on the output of our output layer, so we don't have to implement the softmax layer within the forward() method.\n",
    "\n",
    "Cross Entropy Loss: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "\n",
    "The optimizer object implements a particular algorithm for updating the weights. Here, we will use the Adam optimizer, which is a variant of stochastic gradient descent method that tends to find a better solution in a smaller number of iterations than standard SGD.\n",
    "\n",
    "Optimization: https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "The cell below defines a training function for our classifier. \n",
    "\n",
    "**TO-DO 5:** First, try to follow what each line is doing. Then, modify the training function above to return the training and development (or 'validation') losses at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c682c18-b30c-489f-9c22-fb5662fc7960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "def train_nn(num_epochs, model, train_dataloader, dev_dataloader):\n",
    "    \n",
    "    learning_rate = 0.0005  # learning rate for the gradient descent optimizer, related to the step size\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()  # create loss function object\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # create the optimizer\n",
    "    \n",
    "    all_train_losses = []\n",
    "    all_dev_losses = []\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        # Track performance on the training set as we are learning...\n",
    "        total_correct = 0\n",
    "        total_trained = 0\n",
    "        train_losses = []\n",
    "\n",
    "        model.train()  # Put the model in training mode.\n",
    "\n",
    "        for i, (batch_input_ids, batch_labels) in enumerate(train_dataloader):\n",
    "            # Iterate over each batch of data\n",
    "            # print(f'batch no. = {i}')\n",
    "\n",
    "            optimizer.zero_grad()  # Reset the optimizer\n",
    "\n",
    "            # Use the model to perform forward inference on the input data.\n",
    "            # This will run the forward() function.\n",
    "            output = model(batch_input_ids)\n",
    "\n",
    "            # Compute the loss for the current batch of data\n",
    "            batch_loss = loss_fn(output, batch_labels)\n",
    "\n",
    "            # Perform back propagation to compute the gradients with respect to each weight\n",
    "            batch_loss.backward()\n",
    "\n",
    "            # Update the weights using the compute gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record the loss from this sample to keep track of progress.\n",
    "            train_losses.append(batch_loss.item())\n",
    "\n",
    "            # Count correct labels so we can compute accuracy on the training set\n",
    "            predicted_labels = output.argmax(1)\n",
    "            total_correct += (predicted_labels == batch_labels).sum().item()\n",
    "            total_trained += batch_labels.size(0)\n",
    "\n",
    "        train_accuracy = total_correct/total_trained*100\n",
    "\n",
    "        print(\"Epoch: {}/{}\".format((e+1), num_epochs),\n",
    "              \"Training Loss: {:.4f}\".format(np.mean(train_losses)),\n",
    "              \"Training Accuracy: {:.4f}%\".format(train_accuracy))\n",
    "        \n",
    "        ### ANSWER CODE HERE\n",
    "\n",
    "        ###\n",
    "        \n",
    "        model.eval()  # Switch model to evaluation mode - turn off any random steps such as dropout\n",
    "        total_correct = 0\n",
    "        total_trained = 0\n",
    "        dev_losses = []\n",
    "\n",
    "        for dev_input_ids, dev_labels in dev_dataloader:\n",
    "            ### ANSWER CODE HERE\n",
    "            dev_output = model(dev_input_ids)\n",
    "            dev_loss = loss_fn(dev_output, dev_labels)\n",
    "            \n",
    "            # Save the loss on the dev set\n",
    "            dev_losses.append(dev_loss.item())\n",
    "            #######\n",
    "            \n",
    "            # Count the number of correct predictions\n",
    "            predicted_labels = dev_output.argmax(1)\n",
    "            total_correct += (predicted_labels == dev_labels).sum().item()\n",
    "            total_trained += dev_labels.size(0)\n",
    "            \n",
    "        dev_accuracy = total_correct/total_trained*100\n",
    "        \n",
    "        ### ANSWER CODE HERE\n",
    "\n",
    "        ###        \n",
    "        print(\"Epoch: {}/{}\".format((e+1), num_epochs),\n",
    "              \"Validation Loss: {:.4f}\".format(np.mean(dev_losses)),\n",
    "              \"Validation Accuracy: {:.4f}%\".format(dev_accuracy))\n",
    "\n",
    "    ### MODIFY CODE HERE\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e473f92-78d4-4a5d-94cc-749165e3896e",
   "metadata": {},
   "source": [
    "**TO-DO 6:**  Train the network for 15 epochs and plot the losses by completing the cell below. At which epoch did we get the best model fit? How could we use the dev set losses to return the best model? Remember that neural networks tend to overfit if trained too long, as they have many parameters and are very flexible. \n",
    "\n",
    "Note that the answer can vary each time you run the training process due to random initialisation of the model weights and shuffling of the dataset. \n",
    "\n",
    "ANSWER\n",
    "   * The plot shows the best fit around 5 epochs before it starts to overfit, as the validation loss converges while training loss goes down \n",
    "   * If dev set loss stops going down for a long time, but training set loss keeps decreasing, the model may be overfitting.\n",
    "   * We can stop training at the point where dev set loss stops decreasing \n",
    "   * Or we could save the model with best dev set performance and use that model to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac7c289-fae8-4045-b18c-8db7e678214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "###WRITE YOUR OWN CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d655f07b-c687-4d9b-a0b3-44f0d3ccbead",
   "metadata": {},
   "source": [
    "The code below obtains predictions from our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4368a8-57fc-4cd3-b74e-aa385105c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nn(trained_model, test_loader):\n",
    "\n",
    "    trained_model.eval()  # switch off some randomisation used during training (dropout) to give consistent predictions\n",
    "\n",
    "    correct = 0  # count the number of correct classification labels\n",
    "\n",
    "    gold_labs = []  # gold labels to return\n",
    "    pred_labs = []  # predicted labels to return\n",
    "    \n",
    "    for inputs, labels in test_loader:\n",
    "        test_output = trained_model(inputs)  # run the forward() function on the inputs\n",
    "        predicted_labels = test_output.argmax(1)  # select the class labels with highest logits as our predictions\n",
    "\n",
    "        gold_labs.extend(labels.tolist())\n",
    "        pred_labs.extend(predicted_labels.tolist())\n",
    "    \n",
    "    return gold_labs, pred_labs\n",
    "\n",
    "gold_labs, pred_labs = predict_nn(trained_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5620658-9e5a-4248-aa48-3e3868011aaf",
   "metadata": {},
   "source": [
    "Now, we can use pretrained word embeddings instead of learning them from scratch during training.\n",
    "Here, we will use pretrained GloVe embeddings that were trained on Tweets, as the text domain is the same as our hate speech dataset. The embedding matrix is used to initialise the embedding layer. The code below converts the GloVe embeddings into an embedding matrix suitable for PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f62dca-1ea6-4206-8122-d9f261a7ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "# download the word embeddings from Gensim\n",
    "glove_wv = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872638e9-f86c-48e4-8745-9c667e250a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.zeros((vocab_size, glove_wv.vector_size))\n",
    "\n",
    "glove_wv.vectors\n",
    "\n",
    "count = 0\n",
    "\n",
    "for word in glove_wv.index_to_key:\n",
    "    if word in dictionary.token2id:\n",
    "        word_idx = dictionary.token2id[word]\n",
    "    \n",
    "        embedding_matrix[word_idx, :] = torch.from_numpy(glove_wv[word])\n",
    "        count +=1 \n",
    "        \n",
    "print(embedding_matrix)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850e7656-343f-449c-9329-411ad7fb22d5",
   "metadata": {},
   "source": [
    "The class below extends the FFTextClassifier class to make use of the pretrained embeddings. This means that it inherits all of its functionality, but we overwrite the constructor (the `__init__` method). This way, we don't need to define the forward function again, as it will be the same as before.\n",
    "\n",
    "The embedding layer is now different as it loads pretrained embeddings from our matrix. The argument `freeze` determines whether the embeddings remain fixed to their pretrained values (if `freeze=True`) or are updated through backpropagation to fit them to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b93f97-57b9-4d3b-a36f-547d472e1f37",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FFTextClassifierWithEmbeddings(FFTextClassifier):\n",
    "\n",
    "    def __init__(self, hidden_size, sequence_length, num_classes, embedding_matrix):\n",
    "        super(FFTextClassifier, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_matrix.shape[1] \n",
    "\n",
    "        # Here we just need to construct the components of our network. We don't need to connect them together yet.\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_matrix, freeze=False) # embedding layer\n",
    "\n",
    "        ### COMPLETE THE ARGUMENTS TO SPECIFY THE DIMENSIONS OF THE LAYERS\n",
    "        self.hidden_layer = nn.Linear() # Hidden layer\n",
    "        self.activation = nn.ReLU() # Hidden layer\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes) # Full connection layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae91af-4e37-476d-ba7d-709924bb329c",
   "metadata": {},
   "source": [
    "**TO-DO 7:** Complete the arguments in the `FFTextClassifierWithEmbeddings` constructor to set the dimensions of the neural network layers.  Repeat the experiment above using the FFTextClassifierWithEmbeddings with the GLoVe embeddings and compare the performance of the two neural text classifiers. \n",
    "\n",
    "Note that freezing the pretrained embeddings will speed up the training process, but may make the model less accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4afd4-a92d-468d-a6ea-e693c10de36d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### COMPLETE THE CODE BELOW\n",
    "ff_emb_model = FFTextClassifierWithEmbeddings()\n",
    "trained_emb_model, _, _ = train_nn(5, ff_emb_model, train_loader, dev_loader)\n",
    "gold_labs, pred_labs_emb = predict_nn(trained_emb_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd644ecc-8f10-4a48-ace8-8f95a21d9099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Evaluate the models using F1 score\n",
    "print(f'Random embedding intialisation: {f1_score(gold_labs, pred_labs, average=\"macro\")}')\n",
    "print(f'Frozen pretrained GloVe embeddings: {f1_score(gold_labs, pred_labs_emb, average=\"macro\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b84656-e632-412b-9b14-272c14ebaa45",
   "metadata": {},
   "source": [
    "The model we have tried so far uses fully-connected layers, which means that the weights applied to a particular word's embedding are specific to the location of the word in the sentence -- shifting the tokens along by one could produce a completely different result. We could improve this by using a recurrent neural network layer, that processes the tokens sequentially. The code below adapts our class to use a bidirectional LSTM for the hidden layer(s).\n",
    "\n",
    "**TO-DO 8:** Complete the constructor and forward method below and test the class with the LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e0d0d-ac93-4f7b-8169-c81f6b151eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTextClassifierWithEmbeddings(FFTextClassifier):\n",
    "\n",
    "    def __init__(self, hidden_size, sequence_length, num_classes, embedding_matrix):\n",
    "        super(FFTextClassifier, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_matrix.shape[1] \n",
    "\n",
    "        # Here we just need to construct the components of our network. We don't need to connect them together yet.\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "\n",
    "        self.hidden_layer = nn.LSTM(self.embedding_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        ### COMPLETE THE ARGUMENTS TO SPECIFY THE DIMENSIONS OF THE LAYERS\n",
    "        self.activation = nn.ReLU() # Hidden layer\n",
    "        self.output_layer = nn.Linear() # Full connection layer\n",
    "\n",
    "    def forward (self, input_words):\n",
    "        # Input dimensions are:  (batch_size, seq_length)\n",
    "        embedded_words = self.embedding_layer(input_words)  # (batch_size, seq_length, embedding_size)\n",
    "\n",
    "        # flatten the sequence of embedding vectors for each document into a single vector.\n",
    "        # embedded_words = embedded_words.reshape(embedded_words.shape[0], sequence_length*self.embedding_size)  # batch_size, seq_length*embedding_size\n",
    "        _, (z, _) = self.hidden_layer(embedded_words)   # (2, batch_size, hidden_size)\n",
    "        z = z.swapaxes(0, 1).flatten(1)\n",
    "        \n",
    "        ### ADD THE MISSING LINE HERE\n",
    "\n",
    "        ########\n",
    "        \n",
    "        output = self.output_layer(h)                      # (batch_size, num_classes)\n",
    "\n",
    "        # Notice we haven't applied a softmax activation to the output layer -- it's not required by Pytorch's loss function.\n",
    "\n",
    "        return output\n",
    "\n",
    "# WRITE YOUR CODE HERE TO TEST THE LSTM CLASSIFIER\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c20d5-12c8-4820-89e2-e53c11b77b42",
   "metadata": {},
   "source": [
    "**TO-DO 9:** There are various ways that we could improve the classifier above. Consider what you could change, and try out two simple improvements. Think about the size and number of the hidden layers, the embedding dimensions, and monitoring dev losses to select the best training epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6434ff-a0c0-477c-ae9f-c546ebd09859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
