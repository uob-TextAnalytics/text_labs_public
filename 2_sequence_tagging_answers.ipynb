{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Text Analytics Lab 2: Sequence Labelling\n",
    "\n",
    "In this lab we will implement an HMM for part-of-speech (POS) tagging, then see how to use a library to run an HMM and a CRF for named entity recognition.\n",
    "\n",
    "### Outcomes\n",
    "* Be able to implement the main parts of a supervised HMM.\n",
    "* Understand what the steps of Viterbi are doing.\n",
    "* Know how to use a CRF and specify the features it uses.\n",
    "* Be able to extract named entities using a sequence tagger.\n",
    "\n",
    "### Overview\n",
    "\n",
    "The first part of the notebook loads a POS dataset from the NLTK library.\n",
    "The second part implements and tests an HMM POS tagger.\n",
    "The third part uses CRFs for the task of named entity recognition.\n",
    "\n",
    "## Background\n",
    "\n",
    "### Hidden Markov Model (HMM)\n",
    "\n",
    "In this lab we will look into Hidden Markov Model (HMM) to model sequential data. HMMs are based on the Markov assumption which states that the present state $z_n$ is sufficient to predict the future $y_{n+1}$ so the past $y_{0:n-1}$ can be forgotten.\n",
    "\n",
    "Often, the states we are interested in cannot be observed directly -- they are 'hidden'. As we will see in Exercise 2, the part-of-speech (POS) tags are hidden states that we want to predict. We can only observe the words, and have to use them to infer the tags. \n",
    "An HMM is specified by the following components:\n",
    "\n",
    "* A set of $N$ states.\n",
    "\n",
    "* A transition probability matrix $A$ where each element $a_{ij}$ represents the probability of moving from state $i$ to state $j$, s.t.  $ \\sum^{N}_{j=1} a_{ij} = 1$ $ \\forall i$\n",
    "\n",
    "* An emission probability distribution, the probabilities of observations $x_n$ being generated from a state $y_n$\n",
    "\n",
    "* An initial probability distribution over states. $\\pi_n$ is the probability that the Markov chain will start in state $n$. Also, $\\sum^{N}_{n=1} \\pi_n = 1 $\n",
    "\n",
    "## Part of Speech (POS) Tagging\n",
    "\n",
    "Part-of-speech (PoS) tagging extracts syntactic information about words in a sentence, which can then be used to understand how they relate to neighbouring words. The parts of speech are the grammatical categories of words, such as nouns, verbs, and adjectives. Parts of speech are useful features for information extraction, e.g., for labelling named entities, such as people or organisations, which are often proper nouns. A word’s part of speech can even play a role in speech recognition or speech synthesis, e.g., the word 'content' is pronounced CONtent when it is a noun and conTENT when it is an adjective.\n",
    "\n",
    "PoS tagging assigns a single PoS tag to each token in a text sequence. Therefore, the input to a tagging algorithm is a sequence of tokenised words and the output is a sequence of tags, one per token. Hence, PoS-tagging is a specific case of the more generic NLP task of sequence labelling.\n",
    " Many words can have different PoS tags in different contexts, so the goal is to find the correct tag for a particular usage of a word in a sentence. For example, \"book\" can be a verb (\"book that flight\") or a noun (\"hand me that book\"). The challenge of PoS-tagging is to model the context of a token to resolve these ambiguities and choose the correct tag for the context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing the PoS Tagging Data\n",
    "\n",
    "For PoS tagging, we are going to start with the [Brown corpus](https://www.nltk.org/nltk_data/), which contains many different sources of English text (books, essays, newspaper articles, government documents...) collected and hand-labelled by linguists in 1967.\n",
    "\n",
    "If you would like to try out PoS tagging in another language, you can uncomment the code below to switch to the [NLTK Indian corpus](https://www.nltk.org/_modules/nltk/corpus/reader/indian.html), which contains datasets for POS tagging in Bangla, Hindi, Marathi and Telugu, and you are welcome to use these alternative datasets for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/es1595/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/es1595/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# If you want to try another language, try Hindi:\n",
    "# nltk.download('indian')  # the dataset\n",
    "# from nltk.corpus import indian\n",
    "# nltk_data = list(indian.tagged_sents('hindi.pos'))\n",
    "\n",
    "nltk.download('brown')  # download Brown corpus\n",
    "nltk.download('universal_tagset')   # download the POS tags data\n",
    "from nltk.corpus import brown\n",
    "nltk_data = list(brown.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to split the dataset into training and test sets. Run the following cells to achieve that and to convert the dataset to list format, which will be easier to work with in the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 45872\n",
      "Number of test sentences: 11468\n",
      "Number of training sentences in train_toks: 45872\n",
      "Number of test sentences in test_toks: 11468\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "train_set, test_set = train_test_split(\n",
    "    nltk_data,\n",
    "    train_size=0.80,  # use 80% as the training data\n",
    "    test_size=0.20,\n",
    "    random_state=101\n",
    ")\n",
    "print(f'Number of training sentences: {len(train_set)}')\n",
    "print(f'Number of test sentences: {len(test_set)}')\n",
    "\n",
    "# Separate the labels from the text\n",
    "train_toks = []  # each item in the list is a list of tokens in a document\n",
    "train_tags = []  # each item in the list is a list of corresponding tags\n",
    "for tagged_sentence in train_set:\n",
    "    sentence_toks = []\n",
    "    sentence_tags = []\n",
    "    for token, tag in tagged_sentence:\n",
    "        sentence_toks.append(token)\n",
    "        sentence_tags.append(tag)\n",
    "\n",
    "    train_toks.append(sentence_toks)\n",
    "    train_tags.append(sentence_tags)\n",
    "\n",
    "test_toks = []\n",
    "test_tags = []\n",
    "for tagged_sentence in test_set:\n",
    "    sentence_toks = []\n",
    "    sentence_tags = []\n",
    "    for token, tag in tagged_sentence:\n",
    "        sentence_toks.append(token)\n",
    "        sentence_tags.append(tag)\n",
    "    test_toks.append(sentence_toks)\n",
    "    test_tags.append(sentence_tags)\n",
    "\n",
    "print(f'Number of training sentences in train_toks: {len(train_toks)}')\n",
    "print(f'Number of test sentences in test_toks: {len(test_toks)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many words the vocabulary has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tagged tokens in the training set: 927092\n",
      "Number of tagged tokens in the test set: 234100\n"
     ]
    }
   ],
   "source": [
    "# create list of train and test tagged words\n",
    "print(f'Number of tagged tokens in the training set: {len([ tok for sent in train_toks for tok in sent ])}')\n",
    "print(f'Number of tagged tokens in the test set: {len([ tok for sent in test_toks for tok in sent ])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's expore the different types of tags by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of possible tags: 12\n",
      "Possible tags: {'ADJ', 'NOUN', 'DET', 'VERB', 'X', 'PRT', 'ADP', '.', 'CONJ', 'ADV', 'NUM', 'PRON'}\n"
     ]
    }
   ],
   "source": [
    "unique_tags = {tag for sent in train_tags for tag in sent}\n",
    "print(f'Number of possible tags: {len(unique_tags)}')\n",
    "print(f'Possible tags: {unique_tags}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO-DO 1:** Find out what the tags mean at https://github.com/slavpetrov/universal-pos-tags.\n",
    "\n",
    "The next cell shows an exampes sentence from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence example: [('many', 'ADJ'), ('of', 'ADP'), ('their', 'DET'), ('gifted', 'ADJ'), ('members', 'NOUN'), ('were', 'VERB'), ('prominent', 'ADJ'), ('in', 'ADP'), ('the', 'DET'), ('Vatican', 'NOUN'), ('as', 'ADP'), ('physicians', 'NOUN'), (',', '.'), ('musicians', 'NOUN'), (',', '.'), ('bankers', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print('Sentence example: {}'.format(train_set[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a vocabulary and convert each token to its index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: [41, 28, 46, 40, 42, 47, 45, 23, 31, 37, 38, 44, 11, 43, 11, 39, 0]\n",
      "Size of vocabulary is 56057\n"
     ]
    }
   ],
   "source": [
    "# The text is already tokenized, so we don't use CountVectorizer or other Sklearn tools. Instead, we introduce a class from\n",
    "# the Gensim library to build a vocabulary:\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Convert the tokens to IDs in a vocabulary ready for input to our models\n",
    "dictionary = Dictionary(train_toks + test_toks)\n",
    "\n",
    "train_toks_encoded = [dictionary.doc2idx(sent) for sent in train_toks]\n",
    "test_toks_encoded = [dictionary.doc2idx(sent) for sent in test_toks]\n",
    "print(f'Example sentence: {train_toks_encoded[3]}')\n",
    "\n",
    "V = len(dictionary.values())  # vocabulary\n",
    "print(f'Size of vocabulary is {V}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we convert the tags in our training and test sets to their indexes in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The LabelEncoder maps the tags (e.g., \"ADP\") to numeric IDs (e.g., 2):\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert the tags from their names to numbers\n",
    "tag_encoder = LabelEncoder()\n",
    "tag_encoder.fit([tag for sentence in train_tags for tag in sentence])\n",
    "train_tags_encoded = [tag_encoder.transform(sentence) for sentence in train_tags]\n",
    "test_tags_encoded = [tag_encoder.transform(sentence) for sentence in test_tags]\n",
    "\n",
    "num_tags = len(tag_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of tags: ['.' 'ADJ' 'ADP' 'ADV' 'CONJ' 'DET' 'NOUN' 'NUM' 'PRON' 'PRT' 'VERB' 'X']\n",
      "Mappings from tags to IDs:\n",
      ".: 0\n",
      "ADJ: 1\n",
      "ADP: 2\n",
      "ADV: 3\n",
      "CONJ: 4\n",
      "DET: 5\n",
      "NOUN: 6\n",
      "NUM: 7\n",
      "PRON: 8\n",
      "PRT: 9\n",
      "VERB: 10\n",
      "X: 11\n"
     ]
    }
   ],
   "source": [
    "print(f\"List of tags: {tag_encoder.classes_}\")\n",
    "print(f\"Mappings from tags to IDs:\")\n",
    "for tag in tag_encoder.classes_:\n",
    "    print(f\"{tag}: {tag_encoder.transform([tag])[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2 Implementing the HMM\n",
    "\n",
    "The two main components of HMMs are the transition model and the observation (or emission) model. The transition model estimates $P(tag_{t+1}|tag_t)$, the probability of the next tag given the current tag. For discrete features, such as tokens, the observation model is the same as the naïve Bayes text classifier. It estimates $P(word|tag_t)$, the probability of observing a word given the current tag. This is similar to how naïve Bayes estimates $P(word|class_c)$ for each word that occurs in a document. We can use the same maximum likelihood estimation approach to estimate these probabilities from our training data, by counting how often each word occurs with a given tag.\n",
    "\n",
    "Let's start by implementing the transition matrix. \n",
    "\n",
    "**TO-DO 2:** Count the state (tag) transitions and starting state (tag) occurrences in the training set and store the counts in the `transitions` and `start_states` matrices below. In `transitions`, rows correspond to states at time t-1, the columns to the following state at time t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 45872/45872 [00:00<00:00, 75703.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # progress bars\n",
    "\n",
    "transitions = np.zeros((num_tags, num_tags))  # transition matrix. Initially, we will fill this with counts of transitions\n",
    "start_states = np.zeros(num_tags) # start state probabilities, which we will first fill with counts of how often each state occurs at the start of a sequence\n",
    "\n",
    "for sentence_tags in tqdm(train_tags_encoded):\n",
    "    for i, tag in enumerate(sentence_tags):\n",
    "        if i==0:\n",
    "            start_states[tag] += 1\n",
    "            continue\n",
    "        ### WRITE YOUR OWN CODE HERE\n",
    "        transitions[sentence_tags[i-1], tag] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 3:** Normalise the transition and start state counts to estimate the conditional probabilities in the transition matrix $A$ and starting state probabilities $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "transitions /= np.sum(transitions, 1)[:, None]\n",
    "start_states /= np.sum(start_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's compute the emission/observation model.\n",
    "\n",
    "**TO-DO 4:** Count the number of occurrences of each word type given each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45872it [00:00, 59683.48it/s]\n"
     ]
    }
   ],
   "source": [
    "observations = np.zeros((num_tags, V))  # We will first fill this with counts of words given tags\n",
    "\n",
    "for i, sentence_toks in tqdm(enumerate(train_toks_encoded)):\n",
    "    sentence_tags = train_tags_encoded[i]\n",
    "    for j, tok in enumerate(sentence_toks):\n",
    "        tag = sentence_tags[j]\n",
    "        # WRITE YOUR OWN CODE HERE\n",
    "        observations[tag, tok] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 5:** Normalise the observation counts to obtain the observation probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#WRITE YOUR OWN CODE HERE\n",
    "observations /= np.sum(observations, 1)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To predict the most likely sequence of tags, we use the Viterbi algorithm, defined in the next cell.\n",
    "\n",
    "**TO-DO 6:** Compare this implementation with the slides or textbook and try to understand how it works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def viterbi(observed_seq, num_tags, start_probs, transition_probs, observation_probs):\n",
    "    eps = 1e-7\n",
    "\n",
    "    num_obs = len(observed_seq)\n",
    "\n",
    "    # Initialise the V and backpointers\n",
    "    V = np.zeros((num_obs, num_tags))\n",
    "    backpointer = np.zeros((num_obs, num_tags))\n",
    "\n",
    "    # For the first data point in the sequence:\n",
    "    V[0, :] = start_probs * observation_probs[:, observed_seq[0]]\n",
    "\n",
    "    # Run Viterbi forward for t > 0\n",
    "    for t in range(1, num_obs):\n",
    "\n",
    "        for state in range(num_tags):\n",
    "            # probabilities for all the sequences leading to this state at time t\n",
    "            seq_prob = V[t-1, :] * transition_probs[:, state]\n",
    "\n",
    "            # Choose the most likely sequence\n",
    "            max_seq_prob = np.max(seq_prob)\n",
    "            best_previous_state = np.argmax(seq_prob)\n",
    "\n",
    "            # Calculate the probability of the most likely sequence leading to this state at time t, including the current observation.\n",
    "            # Add eps to help with numerical issues.\n",
    "            V[t, state] = (max_seq_prob + eps) * (observation_probs[state, observed_seq[t]] + eps)\n",
    "\n",
    "            backpointer[t, state] = best_previous_state\n",
    "\n",
    "    t = num_obs - 1\n",
    "\n",
    "    # Initialise the sequence of predicted states\n",
    "    state_seq = np.zeros(num_obs, dtype=int)\n",
    "\n",
    "    # Get the most likely final state:\n",
    "    state_seq[t] = np.argmax(V[t, :])\n",
    "\n",
    "    # Backtrack until the first observation\n",
    "    for t in range(len(observed_seq)-1, 0, -1):\n",
    "        state_seq[t-1] = backpointer[t, state_seq[t]]\n",
    "\n",
    "    return state_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 7:** Use the viterbi function to estimate the most likely sequence of states on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 11468/11468 [00:30<00:00, 372.17it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for sentence in tqdm(test_toks_encoded):\n",
    "    # WRITE YOUR OWN CODE HERE\n",
    "    predictions.append(viterbi(sentence, num_tags, start_states, transitions, observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 11468/11468 [00:01<00:00, 9282.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Viterbi returns a sequence of tag IDs, rather than the actual tags. Now, convert the sequence of tag IDs to tag names:\n",
    "predicted_tags = []\n",
    "for sequence in tqdm(predictions):\n",
    "    predicted_tags.append(tag_encoder.inverse_transform(sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO-DO 8:** Run the code below to print some example predictions. Find some examples where the HMM makes an incorrect prediction. Can you understand why it makes that error? You can use the observation distributions and transition matrix to investigate further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:      ['``', 'My', 'God', ',', \"I'm\", 'shot', \"''\", '!', '!']\n",
      "Gold tag:    ['.', 'DET', 'NOUN', '.', 'PRT', 'VERB', '.', '.', '.']\n",
      "Predictions: ['.' 'DET' 'NOUN' '.' 'PRT' 'NOUN' '.' '.' '.']\n",
      "Tokens:      ['She', 'thought', 'she', 'was', 'bigger', 'than', 'we', 'are', 'because', 'she', 'came', 'from', 'Torino', \"''\", '.']\n",
      "Gold tag:    ['PRON', 'VERB', 'PRON', 'VERB', 'ADJ', 'ADP', 'PRON', 'VERB', 'ADP', 'PRON', 'VERB', 'ADP', 'NOUN', '.', '.']\n",
      "Predictions: ['PRON' 'VERB' 'PRON' 'VERB' 'ADJ' 'ADP' 'PRON' 'VERB' 'ADP' 'PRON' 'VERB'\n",
      " 'ADP' 'NOUN' '.' '.']\n",
      "Tokens:      ['Meanwhile', ',', 'I', 'reloaded', 'my', 'gun', ',', 'as', 'the', 'other', 'men', 'were', 'doing', '.']\n",
      "Gold tag:    ['ADV', '.', 'PRON', 'VERB', 'DET', 'NOUN', '.', 'ADP', 'DET', 'ADJ', 'NOUN', 'VERB', 'VERB', '.']\n",
      "Predictions: ['ADV' '.' 'PRON' 'VERB' 'DET' 'NOUN' '.' 'ADP' 'DET' 'ADJ' 'NOUN' 'VERB'\n",
      " 'VERB' '.']\n",
      "Tokens:      ['The', 'difficulty', 'was', 'that', 'each', 'day', 'seemed', 'to', 'produce', 'its', 'quota', 'of', 'details', 'which', 'must', 'be', 'cleaned', 'up', 'immediately', '.']\n",
      "Gold tag:    ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', 'VERB', 'PRT', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'DET', 'VERB', 'VERB', 'VERB', 'PRT', 'ADV', '.']\n",
      "Predictions: ['DET' 'NOUN' 'VERB' 'ADP' 'DET' 'NOUN' 'VERB' 'PRT' 'VERB' 'DET' 'NOUN'\n",
      " 'ADP' 'NOUN' 'DET' 'VERB' 'VERB' 'VERB' 'PRT' 'ADV' '.']\n"
     ]
    }
   ],
   "source": [
    "# print some examples:\n",
    "examples = [2, 334, 4983, 2389]\n",
    "for eg in examples:\n",
    "    print(f'Tokens:      {test_toks[eg]}')\n",
    "    print(f'Gold tag:    {test_tags[eg]}')\n",
    "    print(f'Predictions: {predicted_tags[eg]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get an overall picture of the HMM's performance by computing accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9436992738146092\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "all_predictions = [tag for sentence in predictions for tag in sentence]\n",
    "all_targets = [tag for sentence in test_tags_encoded for tag in sentence]\n",
    "\n",
    "acc = accuracy_score(all_targets, all_predictions)\n",
    "print(f'Accuracy = {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Named Entity Recognition (NER) with CRF\n",
    "\n",
    "Named entity recognition is the task of identifying mentions of entities in text, such as people, locations, organisations and times. It is usually modelled as a sequence labelling task, where the goal is to label tokens as either a particular entity type or as not a named entity. However, many entities span more than one token, such as the person \"Ada Lovelace\" or the organisation \"University of Bristol\". To show where these spans start and end, we therefore in addition to tagging the entity type, we also tag each token as 'beginning' (at the start of an entity span), 'inside' (continuing an entity span), or 'outside' (not part of an entity). Hence, the complete set of tags include \"O\" for 'outside', along with \"B-Person\", \"I-Person\", \"B-Organisation\", \"I-Organisation\", etc., for each entity type. \n",
    "\n",
    "Let's load some NER data consisting of English news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset with 14041 instances loaded\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "# The data is already divided into training and test sets.\n",
    "# Load the training set:\n",
    "train_dataset = load_dataset(\n",
    "    \"conll2003\",\n",
    "    split=\"train\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset with 3453 instances loaded\n"
     ]
    }
   ],
   "source": [
    "# Load the test set:\n",
    "test_dataset = load_dataset(\n",
    "    \"conll2003\",\n",
    "    split=\"test\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Test dataset with {len(test_dataset)} instances loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of the instances in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as NER tags, the dataset includes PoS tags and Chunk tags, which identify the grammatical phrases in the sentence.\n",
    "\n",
    "The tags are all stored by their indexes, rather than as the tags themselves. The mapping of the PoS tags to their indexes is:\n",
    "\n",
    "```\n",
    "{'\"': 0, \"''\": 1, '#': 2, '$': 3, '(': 4, ')': 5, ',': 6, '.': 7, ':': 8, '``': 9, 'CC': 10, 'CD': 11, 'DT': 12,\n",
    " 'EX': 13, 'FW': 14, 'IN': 15, 'JJ': 16, 'JJR': 17, 'JJS': 18, 'LS': 19, 'MD': 20, 'NN': 21, 'NNP': 22, 'NNPS': 23,\n",
    " 'NNS': 24, 'NN|SYM': 25, 'PDT': 26, 'POS': 27, 'PRP': 28, 'PRP$': 29, 'RB': 30, 'RBR': 31, 'RBS': 32, 'RP': 33,\n",
    " 'SYM': 34, 'TO': 35, 'UH': 36, 'VB': 37, 'VBD': 38, 'VBG': 39, 'VBN': 40, 'VBP': 41, 'VBZ': 42, 'WDT': 43,\n",
    " 'WP': 44, 'WP$': 45, 'WRB': 46}\n",
    " ```\n",
    " \n",
    "The mapping from indexes to NER tags to indexes is:\n",
    "\n",
    "```\n",
    "{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "```\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tag_mapping = {0: 'O', 1:'B-PER', 2:'I-PER', 3:'B-ORG', 4:'I-ORG', 5:'B-LOC', 6:'I-LOC', 7:'B-MISC', 8:'I-MISC'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the NER data in the right format for NLTK's CRFTagger class. The CRFTagger requires that the data is a sequence of tuples, where each tuple contains the input token and the output tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [list(zip(s['tokens'], [ner_tag_mapping[tok] for tok in s['ner_tags']])) for s in train_dataset][:-1]\n",
    "test_set = [list(zip(s['tokens'], [ner_tag_mapping[tok] for tok in s['ner_tags']])) for s in test_dataset][:-1]\n",
    "test_tokens = [s['tokens'] for s in test_dataset][:-1]\n",
    "test_tags = [[ner_tag_mapping[tok] for tok in s['ner_tags']] for s in test_dataset][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train a CRF tagger on our training set. The method you need to use from NLTK is the [train method of the conditional random field (CRF)](https://www.nltk.org/_modules/nltk/tag/crf.html). You need to call the constructor with default arguments, then the train() function.\n",
    "\n",
    "**TO-DO 9:** Write a function to train and return a CRF named entity recogniser.\n",
    "\n",
    "NOTE: for this step, you need to have the package pycrfsuite installed. If you encounter an error saying that this module cannot be found, run ``conda install python-crfsuite``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Train a CRF NER tagger\n",
    "def train_CRF_NER_tagger(train_set):\n",
    "    ### WRITE YOUR OWN CODE HERE\n",
    "    tagger = nltk.tag.CRFTagger()\n",
    "    tagger.train(train_set, 'model.crf.tagger')\n",
    "    return tagger  # return the trained model\n",
    "\n",
    "tagger = train_CRF_NER_tagger(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some predictions from the tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags = tagger.tag_sents(test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well the tagger is performing. In NER, we usually evaluate performance by finding **correctly matched entities, rather than correctly tagged tokens**. Only an exact entity match counts as correct. Therefore, to compute precision, recall and F1 score, we need to compute true positives, false positives and false negatives by looking for the predicted entity spans and the gold-labelled entity spans in the test set.\n",
    "\n",
    "The code below contains a function that extract a list of spans from the tagged sentences. The next function calls extract_spans() and computes the precision, recall and f1 scores. However, the function is incomplete.\n",
    "\n",
    "Run the cal_span_level_F1() function below to compute span-level F1 scores for the predictions. Have a look at the results. Which types of entity are being recognised well and which are very poor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for class LOC = 0.7970501474926254\n",
      "F1 score for class PER = 0.7678398450113012\n",
      "F1 score for class MISC = 0.6956521739130435\n",
      "F1 score for class ORG = 0.6519795657726692\n",
      "Macro-average f1 score = 0.7281304330474099\n"
     ]
    }
   ],
   "source": [
    "def extract_spans(tagged_sents):\n",
    "    \"\"\"\n",
    "    Extract a list of tagged spans for each named entity type, \n",
    "    where each span is represented by a tuple containing the \n",
    "    start token and end token indexes.\n",
    "    \n",
    "    returns: a dictionary containing a list of spans for each entity type.\n",
    "    \"\"\"\n",
    "    spans = {}\n",
    "        \n",
    "    for sidx, sent in enumerate(tagged_sents):\n",
    "        start = -1\n",
    "        entity_type = None\n",
    "        for i, (tok, lab) in enumerate(sent):\n",
    "            if 'B-' in lab:\n",
    "                start = i\n",
    "                end = i + 1\n",
    "                entity_type = lab[2:]\n",
    "            elif 'I-' in lab:\n",
    "                end = i + 1\n",
    "            elif lab == 'O' and start >= 0:\n",
    "                \n",
    "                if entity_type not in spans:\n",
    "                    spans[entity_type] = []\n",
    "                \n",
    "                spans[entity_type].append((start, end, sidx))\n",
    "                start = -1      \n",
    "        # Sometimes an I-token is the last token in the sentence, so we still have to add the span to the list\n",
    "        if start >= 0:    \n",
    "            if entity_type not in spans:\n",
    "                spans[entity_type] = []\n",
    "                \n",
    "            spans[entity_type].append((start, end, sidx))\n",
    "                \n",
    "    return spans\n",
    "\n",
    "\n",
    "def cal_span_level_f1(test_sents, test_sents_with_pred):\n",
    "    # get a list of spans from the test set labels\n",
    "    gold_spans = extract_spans(test_sents)\n",
    "\n",
    "    # get a list of spans predicted by our tagger\n",
    "    pred_spans = extract_spans(test_sents_with_pred)\n",
    "    \n",
    "    # compute the metrics for each class:\n",
    "    f1_per_class = []\n",
    "    \n",
    "    ne_types = gold_spans.keys()  # get the list of named entity types (not the tags)\n",
    "    \n",
    "    for ne_type in ne_types:\n",
    "        # compute the confusion matrix\n",
    "        true_pos = 0\n",
    "        false_pos = 0\n",
    "        \n",
    "        for span in pred_spans[ne_type]:\n",
    "            if span in gold_spans[ne_type]:\n",
    "                true_pos += 1\n",
    "            else:\n",
    "                false_pos += 1\n",
    "                \n",
    "        false_neg = 0\n",
    "        for span in gold_spans[ne_type]:\n",
    "            if span not in pred_spans[ne_type]:\n",
    "                false_neg += 1\n",
    "                \n",
    "        if true_pos + false_pos == 0:\n",
    "            precision = 0\n",
    "        else:\n",
    "            precision = true_pos / float(true_pos + false_pos)\n",
    "            \n",
    "        if true_pos + false_neg == 0:\n",
    "            recall = 0\n",
    "        else:\n",
    "            recall = true_pos / float(true_pos + false_neg)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "            \n",
    "        f1_per_class.append(f1)\n",
    "        print(f'F1 score for class {ne_type} = {f1}')\n",
    "        \n",
    "    print(f'Macro-average f1 score = {np.mean(f1_per_class)}')\n",
    "\n",
    "cal_span_level_f1(test_set, predicted_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to help the CRF tagger by adding some more features -- the great thing about the CRF model is that we can easily add in any features that we think may be helpful, and these do not need to be conditionally independent of one another.\n",
    "\n",
    "In the CRFTagger class, the ```_get_features()``` method extracts the features from the tokens. We can overwrite this method with our own version that provides additional features.\n",
    "\n",
    "**TO-DO 10:** Add in the previous and next works as features. Be careful with the start and end of the sequence where there is no previous or next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata\n",
    "\n",
    "class CustomCRFTagger(nltk.tag.CRFTagger):\n",
    "    _current_tokens = None\n",
    "    \n",
    "    def _get_features(self, tokens, idx):\n",
    "            \"\"\"\n",
    "            Extract basic features about this word including\n",
    "                - Current word\n",
    "                - is it capitalized?\n",
    "                - Does it have punctuation?\n",
    "                - Does it have a number?\n",
    "                - Suffixes up to length 3\n",
    "\n",
    "            Note that : we might include feature over previous word, next word etc.\n",
    "\n",
    "            :return: a list which contains the features\n",
    "            :rtype: list(str)\n",
    "            \"\"\"\n",
    "            token = tokens[idx]\n",
    "\n",
    "            feature_list = []\n",
    "\n",
    "            if not token:\n",
    "                return feature_list\n",
    "\n",
    "            # Capitalization\n",
    "            if token[0].isupper():\n",
    "                feature_list.append(\"CAPITALIZATION\")\n",
    "\n",
    "            # Number\n",
    "            if re.search(self._pattern, token) is not None:\n",
    "                feature_list.append(\"HAS_NUM\")\n",
    "\n",
    "            # Punctuation\n",
    "            punc_cat = {\"Pc\", \"Pd\", \"Ps\", \"Pe\", \"Pi\", \"Pf\", \"Po\"}\n",
    "            if all(unicodedata.category(x) in punc_cat for x in token):\n",
    "                feature_list.append(\"PUNCTUATION\")\n",
    "\n",
    "            # Suffix up to length 3\n",
    "            if len(token) > 1:\n",
    "                feature_list.append(\"SUF_\" + token[-1:])\n",
    "            if len(token) > 2:\n",
    "                feature_list.append(\"SUF_\" + token[-2:])\n",
    "            if len(token) > 3:\n",
    "                feature_list.append(\"SUF_\" + token[-3:])\n",
    "\n",
    "                \n",
    "            # Current word\n",
    "            feature_list.append(\"WORD_\" + token)\n",
    "            \n",
    "            ### WRITE YOUR OWN CODE HERE ###\n",
    "            if idx > 0:\n",
    "                feature_list.append(\"PREVWORD_\" + tokens[idx-1])\n",
    "            if idx < len(tokens)-1:\n",
    "                feature_list.append(\"NEXTWORD_\" + tokens[idx+1])\n",
    "                \n",
    "            ####\n",
    "\n",
    "            return feature_list\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO-DO 11:** Train your custom CRF tagger with your ``_get_features`` method, then test it below. How does it compare to the default tagger? Why do you think adding the new features change the performance in this way? \n",
    "\n",
    "The results show how important it is understand your choice of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a CRF NER tagger\n",
    "def train_CustomCRF_NER_tagger(train_set):\n",
    "    ### WRITE YOUR OWN CODE HERE\n",
    "    tagger = CustomCRFTagger()\n",
    "    tagger.train(train_set, 'model.crf.tagger')\n",
    "    return tagger  # return the trained model\n",
    "\n",
    "tagger = train_CustomCRF_NER_tagger(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for class LOC = 0.8232400860743928\n",
      "F1 score for class PER = 0.8225108225108225\n",
      "F1 score for class MISC = 0.7376425855513309\n",
      "F1 score for class ORG = 0.7019910083493899\n",
      "Macro-average f1 score = 0.771346125621484\n"
     ]
    }
   ],
   "source": [
    "predicted_tags = tagger.tag_sents(test_tokens)\n",
    "cal_span_level_f1(test_set, predicted_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech tags often provide useful information for identifying entities, e.g., by identifying nouns that may be part of a name. Let's see if they help when dealing with the CoNLL 2003 dataset...\n",
    "\n",
    "We can use the PoS tagger from NLTK to tag a sentence, as shown in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/es1595/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "# Download the package for PoS tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Some arbitrary token sequence to see how it works...\n",
    "example_sentence = [\"PoS\", \"tags\", \"often\", \"provide\", \"useful\", \"information\", \"for\", \"identifying\", \"entities\"]\n",
    "\n",
    "# Tag the sentence...\n",
    "pos_tagged_tokens = nltk.pos_tag(example_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PoS', 'NNP'),\n",
       " ('tags', 'NNS'),\n",
       " ('often', 'RB'),\n",
       " ('provide', 'VBP'),\n",
       " ('useful', 'JJ'),\n",
       " ('information', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('identifying', 'VBG'),\n",
       " ('entities', 'NNS')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagged_tokens  # look at the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO-DO 12:** Complete the code below to define another custom CRF tagger that also include PoS tags as features. Then, run the final cells to train and test the CRF tagger with PoS tags. How do the results compare with the previous versions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Improve the CRF NER tagger using parts of speech (see lab 5) as additional features.\n",
    "class CRFTaggerWithPOS(CustomCRFTagger):\n",
    "    _current_tokens = None\n",
    "    \n",
    "    def _get_features(self, tokens, index):\n",
    "        \"\"\"\n",
    "        Extract the features for a token and append the POS tag as an additional feature.\n",
    "        \"\"\"\n",
    "        basic_features = super()._get_features(tokens, index)\n",
    "        \n",
    "        # Get the pos tags for the current sentence and save it\n",
    "        if tokens != self._current_tokens:\n",
    "            self._pos_tagged_tokens = nltk.pos_tag(tokens)\n",
    "            self._current_tokens = tokens\n",
    "            \n",
    "            \n",
    "        ### WRITE YOUR OWN CODE HERE\n",
    "        basic_features.append(self._pos_tagged_tokens[index][1])\n",
    "        ###\n",
    "        \n",
    "        return basic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a CRF NER tagger\n",
    "def train_CRF_NER_tagger_with_POS(train_set):\n",
    "    ### WRITE YOUR OWN CODE HERE\n",
    "    tagger = CRFTaggerWithPOS()\n",
    "    tagger.train(train_set, 'model.crf.tagger')\n",
    "    return tagger  # return the trained model\n",
    "\n",
    "tagger = train_CRF_NER_tagger_with_POS(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for class LOC = 0.8319070904645477\n",
      "F1 score for class PER = 0.8311530270763615\n",
      "F1 score for class MISC = 0.751145038167939\n",
      "F1 score for class ORG = 0.7028227085315573\n",
      "Macro-average f1 score = 0.7792569660601014\n"
     ]
    }
   ],
   "source": [
    "predicted_tags = tagger.tag_sents(test_tokens)\n",
    "cal_span_level_f1(test_set, predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_analytics",
   "language": "python",
   "name": "text_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
